{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import bson\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from copy import deepcopy\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from matplotlib.colors import TABLEAU_COLORS\n",
    "\n",
    "sys.path.append('../../..')\n",
    "from uncertain.utils.data import Data\n",
    "from uncertain.utils.training import train, run_study, load\n",
    "from uncertain.utils.evaluation import test_vanilla, test_uncertain, test_chap_five\n",
    "\n",
    "from uncertain.implicit.base import MF, MLP\n",
    "from uncertain.implicit.extras import HeuristicUncertainty\n",
    "from uncertain.implicit.DoubleMF import JointDoubleMF, SequentialDoubleMF\n",
    "from uncertain.implicit.extras import HeuristicUncertainty\n",
    "from uncertain.implicit.neural import GaussianMLP, MCDropout, Ensemble\n",
    "from uncertain.implicit.bayesianMLP import BayesianMLP\n",
    "\n",
    "if os.path.isfile('data.pkl'):\n",
    "    with open('data.pkl', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    print(f'Printerest data loaded: {data.n_user} users, {data.n_item} items.')\n",
    "    print(f'{len(data.train)} train, {len(data.val)} validation and {len(data.test)} test interactions.')\n",
    "else:\n",
    "    a = pd.read_csv('pinterest-20.train.rating.txt', sep='\\t', header=None)\n",
    "    b = pd.read_csv('pinterest-20.test.rating.txt', sep='\\t', header=None)\n",
    "    data = pd.concat([a, b], ignore_index=True).drop(columns=[2, 3])\n",
    "    data.columns = ['user', 'item']\n",
    "    data = Data(data, implicit=True, distances=False)\n",
    "    with open('data.pkl', 'wb') as f:\n",
    "        pickle.dump(data, f, protocol=4)\n",
    "        \n",
    "data.item_support[data.item_support == 0] = 1\n",
    "data.item_support = data.item_support.astype(float)\n",
    "data.user_support = data.user_support.astype(float)\n",
    "\n",
    "base_batch_size = 1024 *10\n",
    "trials = 0 ## 0 for eval only mode\n",
    "patience = 2 ## Number of validation checks before ending training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the tail items accordind to AUR paper\n",
    "import torch\n",
    "\n",
    "n_inter = data.item_support.sum()\n",
    "order = np.argsort(data.item_support)\n",
    "\n",
    "for n_tail in range(data.n_item):\n",
    "    ratio_tail = np.sum(data.item_support[order[:n_tail]]) / n_inter\n",
    "    if ratio_tail > 0.5:\n",
    "        print(n_tail)\n",
    "        break\n",
    "\n",
    "data.tail_items = torch.tensor(order[:n_tail])\n",
    "data.head_items = torch.tensor(order[n_tail:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "name = 'MF-BCE'\n",
    "def init_model(**kwargs):\n",
    "    return MF(data.n_user, data.n_item, embedding_dim=128, loss='BCE', **kwargs)\n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    # Parameter setup\n",
    "    params = {'lr': trial.suggest_float('lr', 1e-4, 1e-2, log=True),\n",
    "              'weight_decay': trial.suggest_float('wd', 1e-5, 1e-2, log=True),\n",
    "              'n_negatives': trial.suggest_int('neg', 1, 50)}\n",
    "    params_str = '-'.join(f'{key}={value}' for key, value in params.items())\n",
    "    data.batch_size = int(base_batch_size / (params['n_negatives'] + 1))\n",
    "    print(params, data.batch_size)\n",
    "\n",
    "    # Train\n",
    "    model = init_model(**params)\n",
    "    MAP, path, train_likelihood = train(model, data, path='checkpoints/' + name, name=params_str, patience=patience)\n",
    "    trial.set_user_attr('filename', path)\n",
    "    trial.set_user_attr('mean_loss', train_likelihood)\n",
    "    return MAP\n",
    "\n",
    "study = run_study(name, objective, n_trials=trials)\n",
    "best_runs = study.trials_dataframe().sort_values('value')[::-1][:5]\n",
    "mfbce = init_model().load_from_checkpoint(best_runs.user_attrs_filename.iloc[0])\n",
    "\n",
    "results = test_vanilla(mfbce, data, max_k=10, name=name)\n",
    "clear_output(wait=True)\n",
    "print(results['MAP'])\n",
    "best_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'MF-BPR'\n",
    "def init_model(**kwargs):\n",
    "    return MF(data.n_user, data.n_item, embedding_dim=128, loss='BPR', **kwargs)\n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    # Parameter setup\n",
    "    params = {'lr': trial.suggest_float('lr', 1e-4, 1e-2, log=True),\n",
    "              'weight_decay': trial.suggest_float('wd', 1e-5, 1e-2, log=True)}\n",
    "    params_str = '-'.join(f'{key}={value}' for key, value in params.items())\n",
    "    data.batch_size = int(base_batch_size / 2)\n",
    "    print(params, data.batch_size)\n",
    "\n",
    "    # Train\n",
    "    model = init_model(**params)\n",
    "    MAP, path, train_likelihood = train(model, data, path='checkpoints/' + name, name=params_str, patience=patience)\n",
    "    trial.set_user_attr('filename', path)\n",
    "    trial.set_user_attr('mean_loss', train_likelihood)\n",
    "    return MAP\n",
    "\n",
    "study = run_study(name, objective, n_trials=trials)\n",
    "best_runs = study.trials_dataframe().sort_values('value')[::-1]\n",
    "model = init_model().load_from_checkpoint(best_runs.user_attrs_filename.iloc[0])\n",
    "\n",
    "results = test_vanilla(model, data, max_k=10, name=name)\n",
    "clear_output(wait=True)\n",
    "print(results['MAP'])\n",
    "best_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'MF-MSE'\n",
    "def init_model(**kwargs):\n",
    "    return MF(data.n_user, data.n_item, embedding_dim=128, loss='MSE', **kwargs)\n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    # Parameter setup\n",
    "    params = {'lr': trial.suggest_float('lr', 1e-4, 1e-2, log=True),\n",
    "              'weight_decay': trial.suggest_float('wd', 1e-5, 1e-2, log=True),\n",
    "              'n_negatives': trial.suggest_int('neg', 1, 50)}\n",
    "    params_str = '-'.join(f'{key}={value}' for key, value in params.items())\n",
    "    data.batch_size = int(base_batch_size / (params['n_negatives'] + 1))\n",
    "    print(params, data.batch_size)\n",
    "\n",
    "    # Train\n",
    "    model = init_model(**params)\n",
    "    MAP, path, train_likelihood = train(model, data, path='checkpoints/' + name, name=params_str, patience=patience)\n",
    "    trial.set_user_attr('filename', path)\n",
    "    trial.set_user_attr('mean_loss', train_likelihood)\n",
    "    return MAP\n",
    "\n",
    "study = run_study(name, objective, n_trials=0)\n",
    "best_runs = study.trials_dataframe().sort_values('value')[::-1]\n",
    "mfmse = init_model().load_from_checkpoint(best_runs.user_attrs_filename.iloc[0])\n",
    "\n",
    "results = test_vanilla(mfmse, data, max_k=10, name=name)\n",
    "clear_output(wait=True)\n",
    "print(results['MAP'])\n",
    "best_runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'MLP-BCE'\n",
    "def init_model(**kwargs):\n",
    "    return MLP(data.n_user, data.n_item, embedding_dim=128, loss='BCE', **kwargs)\n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    # Parameter setup\n",
    "    params = {'lr': trial.suggest_float('lr', 1e-4, 1e-2, log=True),\n",
    "              'dropout': trial.suggest_float('dropout', 0, 0.2),\n",
    "              'n_negatives': trial.suggest_int('neg', 1, 50)}\n",
    "    params_str = '-'.join(f'{key}={value}' for key, value in params.items())\n",
    "    data.batch_size = int(base_batch_size / (params['n_negatives'] + 1))\n",
    "    print(params, data.batch_size)\n",
    "\n",
    "    # Train\n",
    "    model = init_model(**params)\n",
    "    MAP, path, train_likelihood = train(model, data, path='checkpoints/' + name, name=params_str, patience=patience)\n",
    "    trial.set_user_attr('filename', path)\n",
    "    trial.set_user_attr('mean_loss', train_likelihood)\n",
    "    return MAP\n",
    "\n",
    "study = run_study(name, objective, n_trials=0)\n",
    "best_runs = study.trials_dataframe().sort_values('value')[::-1]\n",
    "mlpbce = init_model().load_from_checkpoint(best_runs.user_attrs_filename.iloc[0])\n",
    "'''\n",
    "results = test_vanilla(mlpbce, data, max_k=10, name=name)\n",
    "clear_output(wait=True)\n",
    "print(results['MAP'])\n",
    "best_runs\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'MLP-BPR'\n",
    "def init_model(**kwargs):\n",
    "    return MLP(data.n_user, data.n_item, embedding_dim=128, loss='BPR', **kwargs)\n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    # Parameter setup\n",
    "    params = {'lr': trial.suggest_float('lr', 1e-4, 1e-2, log=True),\n",
    "              'dropout': trial.suggest_float('dropout', 0, 0.2)}\n",
    "    params_str = '-'.join(f'{key}={value}' for key, value in params.items())\n",
    "    data.batch_size = int(base_batch_size / 2)\n",
    "    print(params, data.batch_size)\n",
    "\n",
    "    # Train\n",
    "    model = init_model(**params)\n",
    "    MAP, path, train_likelihood = train(model, data, path='checkpoints/' + name, name=params_str, patience=patience)\n",
    "    trial.set_user_attr('filename', path)\n",
    "    trial.set_user_attr('mean_loss', train_likelihood)\n",
    "    return MAP\n",
    "\n",
    "study = run_study(name, objective, n_trials=trials)\n",
    "best_runs = study.trials_dataframe().sort_values('value')[::-1]\n",
    "model = init_model().load_from_checkpoint(best_runs.user_attrs_filename.iloc[0])\n",
    "\n",
    "results = test_vanilla(model, data, max_k=10, name=name)\n",
    "clear_output(wait=True)\n",
    "print(results['MAP'])\n",
    "best_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'MLP-MSE'\n",
    "def init_model(**kwargs):\n",
    "    return MLP(data.n_user, data.n_item, loss='MSE', **kwargs)\n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    # Parameter setup\n",
    "    params = {'lr': trial.suggest_float('lr', 1e-4, 1e-2, log=True),\n",
    "              'dropout': trial.suggest_float('dropout', 0, 0.2),\n",
    "              'n_negatives': trial.suggest_int('neg', 1, 50)}\n",
    "    params_str = '-'.join(f'{key}={value}' for key, value in params.items())\n",
    "    data.batch_size = int(base_batch_size / (params['n_negatives'] + 1))\n",
    "    print(params, data.batch_size)\n",
    "\n",
    "    # Train\n",
    "    model = init_model(**params)\n",
    "    MAP, path, train_likelihood = train(model, data, path='checkpoints/' + name, name=params_str, patience=patience)\n",
    "    trial.set_user_attr('filename', path)\n",
    "    trial.set_user_attr('mean_loss', train_likelihood)\n",
    "    return MAP\n",
    "    \n",
    "study = run_study(name, objective, n_trials=0)\n",
    "best_runs = study.trials_dataframe().sort_values('value')[::-1]\n",
    "model = init_model().load_from_checkpoint(best_runs.user_attrs_filename.iloc[0])\n",
    "\n",
    "results = test_vanilla(model, data, max_k=10, name=name)\n",
    "clear_output(wait=True)\n",
    "print(results['MAP'])\n",
    "best_runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load results\n",
    "results = {}\n",
    "for key in os.listdir('results'):\n",
    "    results[key.replace('.pkl', '').replace('_', ' ')] = pickle.load(open(os.path.join('results', key), 'rb'))\n",
    "order = ['MF-MSE', 'MF-BCE', 'MF-BPR', 'MLP-MSE', 'MLP-BCE', 'MLP-BPR']\n",
    "results = pd.DataFrame([results[key] for key in order], index=order)\n",
    "\n",
    "# Plot aestetics\n",
    "colors = [c for c in list(TABLEAU_COLORS)]\n",
    "colors = {k:c for k, c in zip(results.index, colors)}\n",
    "lines = ['o', 'v', '^', '<', '>', 's', 'p', '+', 'x', '*', 'p']\n",
    "lines = {k: '-' + l for k, l, in zip(results.index, lines)}\n",
    "\n",
    "# Results\n",
    "print(results['FCP'])\n",
    "\n",
    "## Top-K accuracy metrics\n",
    "f, ax = plt.subplots(figsize=(10, 5), ncols=2)\n",
    "for index, row in results.iterrows():\n",
    "    ax[0].plot(np.arange(1, 11), row['MAP'], lines[index], color=colors[index], label=index, linewidth=3, alpha=0.6)\n",
    "    ax[1].plot(np.arange(1, 11), row['Recall'], lines[index], color=colors[index], label=index, linewidth=3, alpha=0.6)\n",
    "ax[0].set_xticks(np.arange(1, 11))\n",
    "ax[0].set_xlabel('n', fontsize=20)\n",
    "ax[0].set_ylabel('MAP@n', fontsize=20)\n",
    "ax[1].set_xticks(np.arange(1, 11))\n",
    "ax[1].set_xlabel('n', fontsize=20)\n",
    "ax[1].set_ylabel('Recall@n', fontsize=20)\n",
    "handles, labels = ax[0].get_legend_handles_labels()\n",
    "f.legend(handles, labels, fontsize=15, ncol=3, bbox_to_anchor=(0.83, 1.15))\n",
    "f.tight_layout()\n",
    "f.savefig('plots/baselines.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User / Item support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = test_uncertain(HeuristicUncertainty(baseline=mfmse, user_uncertainty=-data.user_support), data, name='NUS', max_k=10)\n",
    "print(results['MAP-Uncertainty'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = test_uncertain(HeuristicUncertainty(baseline=mfmse, item_uncertainty=-data.item_support), data, name='NIS', max_k=10)\n",
    "print(results['MAP-Uncertainty'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load params\n",
    "best_params = study.best_params\n",
    "data.batch_size = int(base_batch_size / (best_params['neg'] + 1))\n",
    "best_params = {'n_negatives': best_params['neg'], 'weight_decay': best_params['wd']}\n",
    "\n",
    "# Train\n",
    "if trials > 0:\n",
    "    for i in range(4):\n",
    "        model = init_model(**best_params)\n",
    "        train(model, data, path='checkpoints/ensemble_MF', name=f'{i}')\n",
    "\n",
    "# Load ensemble models\n",
    "models = [mfmse]\n",
    "for file in os.listdir('checkpoints/ensemble_MF'):\n",
    "    models.append(init_model())\n",
    "    models[-1] = models[-1].load_from_checkpoint(os.path.join('checkpoints/ensemble_MF', file))\n",
    "    models[-1].eval()\n",
    "ensemble = Ensemble(models)\n",
    "clear_output(wait=True)\n",
    "results = test_uncertain(ensemble, data, name='MF-ENSEMBLE', max_k=10)\n",
    "print(results['MAP'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MC Dropout\n",
    "dropout = MCDropout(base_model=mlpbce, mc_iteration=5)\n",
    "results = test_uncertain(dropout, data, max_k=10, name='MCDropout')\n",
    "print(results['MAP-Uncertainty'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['Cuts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BayesianNN\n",
    "name = 'BayesianMLP'\n",
    "def init_model(**kwargs):\n",
    "    return BayesianMLP(data.n_user, data.n_item, embedding_dim=128, **kwargs)\n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    # Parameter setup\n",
    "    params = {'lr': trial.suggest_float('lr', 1e-4, 1e-2, log=True),\n",
    "              'n_negatives': trial.suggest_int('neg', 1, 20),\n",
    "              'sample_train': trial.suggest_int('train_samples', 1, 10),\n",
    "              'prior_pi': trial.suggest_categorical('pi', [1/4, 1/2, 3/4]),\n",
    "              'prior_sigma_1': trial.suggest_categorical('sigma1', np.exp(-np.array([0, 1, 2]))),\n",
    "              'prior_sigma_2': trial.suggest_categorical('sigma2', np.exp(-np.array([6, 7, 8])))}\n",
    "    data.batch_size = int(base_batch_size / (params['n_negatives'] + 1))\n",
    "    params['num_batches'] = int(len(data.train) / data.batch_size)\n",
    "    params_str = '-'.join(f'{key}={value}' for key, value in params.items())\n",
    "    print(params, data.batch_size)\n",
    "    \n",
    "    # Train\n",
    "    model = init_model(**params)\n",
    "    MAP, path, train_likelihood = train(model, data, path='checkpoints/' + name, name=params_str, patience=patience)\n",
    "    trial.set_user_attr('filename', path)\n",
    "    trial.set_user_attr('mean_loss', train_likelihood)\n",
    "    return MAP\n",
    "\n",
    "study = run_study(name, objective, n_trials=1)\n",
    "best_runs = study.trials_dataframe().sort_values('value')[::-1]\n",
    "model = init_model().load_from_checkpoint(best_runs.user_attrs_filename.iloc[0])\n",
    "\n",
    "results = test_vanilla(model, data, max_k=10, name=name)\n",
    "clear_output(wait=True)\n",
    "print(results['MAP'])\n",
    "best_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Deep Ensemble\n",
    "# Load params\n",
    "best_params = study.best_params\n",
    "data.batch_size = int(base_batch_size / (best_params['neg'] + 1))\n",
    "best_params = {'n_negatives': best_params['neg'], 'dropout': best_params['dropout']}\n",
    "\n",
    "# Train\n",
    "if trials > 0:\n",
    "    for i in range(4):\n",
    "        model = init_model(**best_params)\n",
    "        train(model, data, path='checkpoints/ensemble_MLP', name=f'{i}')\n",
    "\n",
    "# Load ensemble models\n",
    "models = [mlpbce]\n",
    "for file in os.listdir('checkpoints/ensemble_MLP'):\n",
    "    models.append(MLP(data.n_user, data.n_item))\n",
    "    models[-1] = models[-1].load_from_checkpoint(os.path.join('checkpoints/ensemble_MLP', file))\n",
    "    models[-1].eval()\n",
    "ensemble = Ensemble(models)\n",
    "clear_output(wait=True)\n",
    "results = test_uncertain(ensemble, data, name='MLP-ENSEMBLE', max_k=10)\n",
    "print(results['MAP-Uncertainty'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['MF-ENSEMBLE'].pop('norm_unc')\n",
    "results['MF-ENSEMBLE'].keys(), results['MF-ENSEMBLE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Baseline results\n",
    "results = {}\n",
    "for key in os.listdir('results'):\n",
    "    results[key.replace('.pkl', '').replace('_', ' ')] = pickle.load(open(os.path.join('results', key), 'rb'))\n",
    "order = ['MF-MSE', 'MF-BCE', 'MF-BPR', 'MLP-MSE', 'MLP-BCE', 'MLP-BPR']\n",
    "results = pd.DataFrame([results[key] for key in order], index=order)\n",
    "\n",
    "# Plot aestetics\n",
    "colors = [c for c in list(TABLEAU_COLORS)]\n",
    "colors = {k:c for k, c in zip(results.index, colors)}\n",
    "lines = ['o', 'v', '^', '<', '>', 's', 'p', '+', 'x', '*', 'p']\n",
    "lines = {k: '-' + l for k, l, in zip(results.index, lines)}\n",
    "\n",
    "## Baselines\n",
    "f, ax = plt.subplots(figsize=(10, 5), ncols=2)\n",
    "for index, row in results.iterrows():\n",
    "    ax[0].plot(np.arange(1, 11), row['MAP'], lines[index], color=colors[index], label=index, linewidth=3, alpha=0.6)\n",
    "    ax[1].plot(np.arange(1, 11), row['Recall'], lines[index], color=colors[index], label=index, linewidth=3, alpha=0.6)\n",
    "ax[0].set_xticks(np.arange(1, 11))\n",
    "ax[0].set_xlabel('n', fontsize=20)\n",
    "ax[0].set_ylabel('MAP@n', fontsize=20)\n",
    "ax[1].set_xticks(np.arange(1, 11))\n",
    "ax[1].set_xlabel('n', fontsize=20)\n",
    "ax[1].set_ylabel('Recall@n', fontsize=20)\n",
    "handles, labels = ax[0].get_legend_handles_labels()\n",
    "f.legend(handles, labels, fontsize=15, ncol=3, bbox_to_anchor=(0.83, 1.15))\n",
    "f.tight_layout()\n",
    "f.savefig('plots/baselines4.pdf', bbox_inches='tight')\n",
    "\n",
    "# Uncertainty models results\n",
    "results = {}\n",
    "for key in os.listdir('results'):\n",
    "    results[key.replace('.pkl', '').replace('_', ' ')] = pickle.load(open(os.path.join('results', key), 'rb'))\n",
    "order = ['MF-BCE', 'MLP-MSE', 'NIS', 'MF-ENSEMBLE', 'MLP-ENSEMBLE', 'MCDropout', 'BayesianMLP']\n",
    "results = pd.DataFrame([results[key] for key in order], index=order)\n",
    "results.rename(index={'NIS': 'NEG-ITEM-SUPPORT', 'MF-BCE': 'MF', 'MLP-MSE': 'MLP'}, inplace=True)\n",
    "\n",
    "# Plot aestetics\n",
    "colors = [c for c in list(TABLEAU_COLORS)]\n",
    "colors = {k:c for k, c in zip(results.index, colors)}\n",
    "lines = ['o', 'v', '^', '<', '>', 's', 'p', '+', 'x', '*', 'p']\n",
    "lines = {k: '-' + l for k, l, in zip(results.index, lines)}\n",
    "\n",
    "# Correlation plot\n",
    "f, ax = plt.subplots(figsize=(8, 5), ncols=1)\n",
    "corr = results[['corr_usup', 'corr_isup']].drop(index=['MF', 'MLP'])\n",
    "corr.columns=[r'#$R_{u\\cdot}$', r'#$R_{\\cdot i}$']\n",
    "corr.loc['NEG-ITEM-SUPPORT', r'#$R_{\\cdot i}$'] = np.nan\n",
    "sns.heatmap(corr.round(3), annot=True, vmax=1, vmin=-1, center=0, cmap='vlag')\n",
    "plt.yticks(fontsize=15)\n",
    "plt.xticks(rotation=45, fontsize=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/corr4.pdf')\n",
    "\n",
    "## Top-K accuracy metrics\n",
    "f, ax = plt.subplots(figsize=(10, 5), ncols=2)\n",
    "for index, row in results.iterrows():\n",
    "    if index != 'NEG-ITEM-SUPPORT':\n",
    "        ax[0].plot(np.arange(1, 11), row['MAP'], lines[index], color=colors[index], label=index, linewidth=3, alpha=0.6)\n",
    "        ax[1].plot(np.arange(1, 11), row['Recall'], lines[index], color=colors[index], label=index, linewidth=3, alpha=0.6)\n",
    "ax[0].set_xticks(np.arange(1, 11))\n",
    "ax[0].set_xlabel('n', fontsize=20)\n",
    "ax[0].set_ylabel('MAP@n', fontsize=20)\n",
    "ax[1].set_xticks(np.arange(1, 11))\n",
    "ax[1].set_xlabel('n', fontsize=20)\n",
    "ax[1].set_ylabel('Recall@n', fontsize=20)\n",
    "handles, labels = ax[0].get_legend_handles_labels()\n",
    "f.legend(handles, labels, fontsize=15, ncol=3, bbox_to_anchor=(0.86, 1.15))\n",
    "f.tight_layout()\n",
    "f.savefig('plots/accuracy4.pdf', bbox_inches='tight')\n",
    "\n",
    "# MAP vs Uncertainty\n",
    "results = results.drop(index=['MF', 'MLP'])\n",
    "f, ax = plt.subplots(figsize=(8, 5))\n",
    "x = np.arange(10) + 1\n",
    "for index, row in results.iterrows():\n",
    "    ax.plot(x, row['MAP-Uncertainty'], lines[index], color=colors[index], label=index, linewidth=3, alpha=0.6)\n",
    "ax.set(xticks = x)\n",
    "ax.set_xlabel('Uncertainty bin', fontsize=25)\n",
    "ax.set_ylabel('MAP@10', fontsize=20)\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "f.legend(handles, labels, fontsize=15, ncol=3, bbox_to_anchor=(1.038, 1.15))\n",
    "f.tight_layout()\n",
    "f.savefig('plots/MAP-Uncertainty.pdf', bbox_inches='tight')\n",
    "\n",
    "# Cuts\n",
    "indexs = [index for index in order if index != 'Baseline']\n",
    "f, ax = plt.subplots(nrows=3, ncols=1, figsize=(5, 10))\n",
    "for index, row in results.iterrows():\n",
    "    ax[0].plot(row['Cuts']['MAP'], lines[index], color=colors[index], label=index, linewidth=3, alpha=0.6)\n",
    "    ax[1].plot(1/(1 + np.exp(-row['Cuts']['Relevance'])), lines[index], color=colors[index], label=index, linewidth=3, alpha=0.6)\n",
    "    ax[2].plot(row['Cuts']['Coverage'], lines[index], color=colors[index], label=index, linewidth=3, alpha=0.6)\n",
    "ax[0].set_xticks(range(5))\n",
    "ax[0].set_xticklabels(np.linspace(1, 0.2, 5).round(2))\n",
    "ax[0].set_xlabel('Uncertainty quantile cut', fontsize=20)\n",
    "ax[0].set_ylabel('MAP@10', fontsize=20)\n",
    "ax[1].set_xticks(range(5))\n",
    "ax[1].set_xticklabels(np.linspace(1, 0.2, 5).round(2))\n",
    "ax[1].set_xlabel('Uncertainty quantile cut', fontsize=20)\n",
    "ax[1].set_ylabel('Mean Predicted Relevance@10', fontsize=20)\n",
    "ax[2].set_xticks(range(5))\n",
    "ax[2].set_xticklabels(np.linspace(1, 0.2, 5).round(2))\n",
    "ax[2].set_xlabel('Uncertainty quantile cut', fontsize=20)\n",
    "ax[2].set_ylabel('Coverage', fontsize=20)\n",
    "# handles, labels = ax[0].get_legend_handles_labels()\n",
    "# f.legend(handles, labels, fontsize=15, bbox_to_anchor=(1.3, 1.1), ncol=5)\n",
    "f.tight_layout()\n",
    "f.savefig('plots/cuts4.pdf', bbox_inches=\"tight\")\n",
    "\n",
    "# UAC / URI\n",
    "results[['UAC', 'URI']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DoubleMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "name = 'AUR_final'\n",
    "def init_model(**kwargs):\n",
    "    return JointDoubleMF(data.n_user, data.n_item, embedding_dim=128, embedding_dim_var=128, beta=1e-2, ratio=0.9, loss='AUR', **kwargs)\n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    # Parameter setup\n",
    "    params = {'lr': trial.suggest_float('lr', 1e-5, 1e-3, log=True),\n",
    "              'beta': trial.suggest_float('beta', 1e-3, 1, log=True),\n",
    "              'gamma': trial.suggest_float('gamma', 1e-4, 1e-1, log=True),\n",
    "              'weight_decay': trial.suggest_float('wd', 1e-5, 1e-3),\n",
    "              'n_negatives': trial.suggest_int('neg', 1, 30)}\n",
    "    params_str = '-'.join(f'{key}={value}' for key, value in params.items())\n",
    "    data.batch_size = int(base_batch_size / (params['n_negatives'] + 1))\n",
    "    print(params, data.batch_size)\n",
    "\n",
    "    # Train\n",
    "    model = init_model(**params)\n",
    "    MAP, path, train_likelihood = train(model, data, path='checkpoints/' + name, name=params_str, patience=patience)\n",
    "    trial.set_user_attr('filename', path)\n",
    "    trial.set_user_attr('mean_loss', train_likelihood)\n",
    "\n",
    "    # Rename model file\n",
    "    os.rename('checkpoints/' + name + '/last.ckpt', 'checkpoints/' + name + '/' + params_str + '_last.ckpt')\n",
    "    \n",
    "    return MAP\n",
    "\n",
    "study = run_study(name, objective, n_trials=0)\n",
    "best_runs = study.trials_dataframe().sort_values('value')[::-1][:5]\n",
    "aur = init_model().load_from_checkpoint(best_runs.user_attrs_filename.iloc[0])\n",
    "# results = test_chap_five(aur, data, max_k=10, name=name)\n",
    "\n",
    "clear_output(wait=True)\n",
    "best_runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ICPMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'CPMF_final'\n",
    "def init_model(**kwargs):\n",
    "    return JointDoubleMF(data.n_user, data.n_item, embedding_dim=128, embedding_dim_var=1, beta=1/2, ratio=0.9, loss='AUR', **kwargs)\n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    # Parameter setup\n",
    "    params = {'lr': trial.suggest_float('lr', 1e-5, 1e-4, log=True),\n",
    "              'gamma': trial.suggest_float('gamma', 1e-4, 1e-1, log=True),\n",
    "              'weight_decay': trial.suggest_float('wd', 1e-6, 1e-4),\n",
    "              'n_negatives': trial.suggest_int('neg', 1, 30)}\n",
    "    params_str = '-'.join(f'{key}={value}' for key, value in params.items())\n",
    "    data.batch_size = int(base_batch_size / (params['n_negatives'] + 1))\n",
    "    print(params, data.batch_size)\n",
    "\n",
    "    # Train\n",
    "    model = init_model(**params)\n",
    "    MAP, path, train_likelihood = train(model, data, path='checkpoints/' + name, name=params_str, patience=patience)\n",
    "    print(MAP, path, train_likelihood)\n",
    "    trial.set_user_attr('filename', path)\n",
    "    trial.set_user_attr('mean_loss', train_likelihood)\n",
    "\n",
    "    # Rename model file\n",
    "    os.rename('checkpoints/' + name + '/last.ckpt', 'checkpoints/' + name + '/' + params_str + '_last.ckpt')\n",
    "    \n",
    "    return MAP\n",
    "\n",
    "study = run_study(name, objective, n_trials=0)\n",
    "# best_runs = study.trials_dataframe().sort_values('value')[::-1][:5]\n",
    "# model = init_model().load_from_checkpoint(best_runs.user_attrs_filename.iloc[0])\n",
    "icpmf = init_model().load_from_checkpoint('/home/vcoscrato/Documents/RecSys/MF/tests/Pointwise/Pinterest/checkpoints/CPMF_final/lr=2.2180734217155113e-05-gamma=0.0012755765912784212-weight_decay=6.320190751204811e-05-n_negatives=19-epoch=69-val_MAP=0.08589229732751846.ckpt')\n",
    "# results = test_chap_five(icpmf, data, max_k=10, name=name)\n",
    "\n",
    "clear_output(wait=True)\n",
    "# best_runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "name = 'Neural-AUR_final'\n",
    "def init_model(**kwargs):\n",
    "    return GaussianMLP(data.n_user, data.n_item, embedding_dim=128, beta=1e-2, ratio=0.9, loss='AUR', **kwargs)\n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    # Parameter setup\n",
    "    params = {'lr': trial.suggest_float('lr', 1e-5, 1e-3, log=True),\n",
    "              'beta': trial.suggest_float('beta', 1e-3, 1, log=True),\n",
    "              'gamma': trial.suggest_float('gamma', 1e-4, 1e-1, log=True),\n",
    "              'dropout': trial.suggest_float('dropout', 0, 0.2),\n",
    "              'n_negatives': trial.suggest_int('neg', 1, 30)}\n",
    "    params_str = '-'.join(f'{key}={value}' for key, value in params.items())\n",
    "    data.batch_size = int(base_batch_size / (params['n_negatives'] + 1))\n",
    "    print(params, data.batch_size)\n",
    "\n",
    "    # Train\n",
    "    model = init_model(**params)\n",
    "    MAP, path, train_likelihood = train(model, data, path='checkpoints/' + name, name=params_str, patience=patience)\n",
    "    trial.set_user_attr('filename', path)\n",
    "    trial.set_user_attr('mean_loss', train_likelihood)\n",
    "    return MAP\n",
    "\n",
    "study = run_study(name, objective, n_trials=trials)\n",
    "best_runs = study.trials_dataframe().sort_values('value')[::-1][:5]\n",
    "gaussianMLP = init_model().load_from_checkpoint(best_runs.user_attrs_filename.iloc[0])\n",
    "gaussianMLP.dropl.eval()\n",
    "# results = test_chap_five(gaussianMLP, data, max_k=10, name=name)\n",
    "\n",
    "clear_output(wait=True)\n",
    "best_runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Bayesian Ranking (HBR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uncertain.implicit.pyro import HBR, train, visualize\n",
    "\n",
    "hbr = HBR(data.n_user, data.n_item, embedding_dim=128)\n",
    "visualize(hbr, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'HBR_final'\n",
    "def init_model(**kwargs):\n",
    "    return HBR(data.n_user, data.n_item, embedding_dim=128, **kwargs)\n",
    "\n",
    "def objective(trial):\n",
    "    pyro.clear_param_store()\n",
    "    \n",
    "    # Parameter setup\n",
    "    tau_ui = trial.suggest_float('tau_gamma', 1e-5, 1e-1, log=True)\n",
    "    params = {'lr': trial.suggest_float('lr', 1e-5, 1e-3, log=True),\n",
    "              'tau_gamma': trial.suggest_float('tau_gamma', 1e-5, 1e-1, log=True),\n",
    "              'tau_u': tau_ui,\n",
    "              'tau_i': tau_ui,\n",
    "              'n_negatives': trial.suggest_int('neg', 1, 30)}\n",
    "    params_str = '-'.join(f'{key}={value}' for key, value in params.items())\n",
    "    data.batch_size = int(base_batch_size / (params['n_negatives'] + 1))\n",
    "    print(params, data.batch_size)\n",
    "\n",
    "    # Train\n",
    "    model = init_model(**params)\n",
    "    MAP = train(model, data, n_steps=200, val_every_n_epochs=2)\n",
    "    filename = 'checkpoints/Bern_CPMF/' + params_str + '.pkl'\n",
    "\n",
    "    # Save model params\n",
    "    with open(filename, 'wb') as f:\n",
    "        params = {'user_embeddings': model.user_embeddings,\n",
    "                  'item_embeddings': model.item_embeddings,\n",
    "                  'user_var': model.user_var,\n",
    "                  'item_var': model.item_var}\n",
    "        pickle.dump(params, f, protocol=5)\n",
    "    \n",
    "    trial.set_user_attr('filename', filename)\n",
    "    return MAP\n",
    "\n",
    "study = run_study(name, objective, n_trials=0)\n",
    "best_runs = study.trials_dataframe().sort_values('value')[::-1]\n",
    "best_model_path = study.trials_dataframe().sort_values('value')['user_attrs_filename'].iloc[-1]\n",
    "with open(best_model_path, 'rb') as f:\n",
    "    params = pickle.load(f)\n",
    "    bern_cpmf = init_model()\n",
    "    bern_cpmf.user_embeddings = params['user_embeddings']\n",
    "    bern_cpmf.item_embeddings = params['item_embeddings']\n",
    "    bern_cpmf.user_var = params['user_var']\n",
    "    bern_cpmf.item_var = params['item_var']\n",
    "\n",
    "results = test_chap_five(bern_cpmf, data, max_k=10, name='HBR-R', debug=True)\n",
    "clear_output(wait=True)\n",
    "# print(results['MAP'].mean(0)[0], results['MAP'].mean(0)[-1])\n",
    "\n",
    "import copy\n",
    "hbrY = copy.copy(bern_cpmf)\n",
    "hbrY.interact = hbrY.interact_return_logit_normal\n",
    "\n",
    "results = test_chap_five(hbrY, data, max_k=10, name='HBR-Y', debug=True)\n",
    "clear_output(wait=True)\n",
    "print(results['MAP'].mean(0)[0], results['MAP'].mean(0)[-1])\n",
    "\n",
    "best_runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = {}\n",
    "for key in os.listdir('results'):\n",
    "    results[key.replace('.pkl', '').replace('_final', '')] = pickle.load(open(os.path.join('results', key), 'rb'))\n",
    "order = ['MF-MSE', 'AUR', 'CPMF', 'Neural-AUR', 'HBR', 'HBR-Y']\n",
    "results = pd.DataFrame([results[key] for key in order], index=order)\n",
    "results.rename(index={'Neural-AUR': 'Gaussian-MLP', 'CPMF': 'ICPMF', 'HBR': 'HBR-R', 'MF-MSE': 'MF (Baseline)'}, inplace=True)\n",
    "\n",
    "# Plot aestetics\n",
    "colors = [c for c in list(TABLEAU_COLORS)]\n",
    "colors = {k:c for k, c in zip(results.index, colors)}\n",
    "lines = ['o', 'v', '^', '<', '>', 's', 'p', '+', 'x', '*', 'p']\n",
    "lines = {k: '-' + l for k, l, in zip(results.index, lines)}\n",
    "\n",
    "## Baselines\n",
    "f, ax = plt.subplots(figsize=(10, 4), ncols=2)\n",
    "for index, row in results.iterrows():\n",
    "    if index == 'MF (Baseline)':\n",
    "        ax[0].plot(np.repeat(row['MAP'][-1], 11), '--', color=colors[index], label=index, linewidth=3, alpha=0.6)\n",
    "        ax[1].plot(np.repeat(row['Recall'][-1], 11), '--', color=colors[index], label=index, linewidth=3, alpha=0.6)\n",
    "    else:\n",
    "        ax[0].plot(row['MAP'].mean(0)[:, -1], lines[index], color=colors[index], label=index, linewidth=3, alpha=0.6)\n",
    "        ax[1].plot(row['Recall'].mean(0)[:, -1], lines[index], color=colors[index], label=index, linewidth=3, alpha=0.6)\n",
    "for line in [0, 1]:\n",
    "    ax[line].set_xticks(np.arange(11))\n",
    "    ax[line].set_xticklabels(np.linspace(0, 1, 11).round(2))\n",
    "    ax[line].set_xlabel(r'$\\lambda$', fontsize=20)\n",
    "ax[0].set_ylabel('MAP@10', fontsize=20)\n",
    "ax[1].set_ylabel('Recall@10', fontsize=20)\n",
    "handles, labels = ax[0].get_legend_handles_labels()\n",
    "# f.legend(handles, labels, fontsize=15, ncol=3, bbox_to_anchor=(0.851, 1.2))\n",
    "f.tight_layout()\n",
    "f.savefig('plots/accuracy5.pdf', bbox_inches='tight')\n",
    "\n",
    "\n",
    "f, ax = plt.subplots(figsize=(10, 4), ncols=2)\n",
    "for index, row in results.iterrows():\n",
    "    if index == 'MF (Baseline)':\n",
    "        print(row['Map relative'])\n",
    "        ax[0].plot(np.repeat(row['Map relative'][-1] -0.045, 11), '--', color=colors[index], label=index, linewidth=3, alpha=0.6)\n",
    "        ax[1].plot(np.repeat(row['Recall relative'][-1] -0.111, 11), '--', color=colors[index], label=index, linewidth=3, alpha=0.6)\n",
    "    else:\n",
    "        ax[0].plot(np.nanmean(row['Map relative'], 0)[:, -1], lines[index], color=colors[index], label=index, linewidth=3, alpha=0.6)\n",
    "        ax[1].plot(np.nanmean(row['Recall relative'], 0)[:, -1], lines[index], color=colors[index], label=index, linewidth=3, alpha=0.6)\n",
    "for line in [0, 1]:\n",
    "    ax[line].set_xticks(np.arange(0, 11))\n",
    "    ax[line].set_xticklabels(np.linspace(0, 1, 11).round(2))\n",
    "    ax[line].set_xlabel(r'$\\lambda$', fontsize=20)\n",
    "ax[0].set_ylabel('MAP Relative@10', fontsize=20)\n",
    "ax[1].set_ylabel('Recall Relative@10', fontsize=20)\n",
    "handles, labels = ax[0].get_legend_handles_labels()\n",
    "# f.legend(handles, labels, fontsize=15, ncol=3, bbox_to_anchor=(0.851, 1.2))\n",
    "f.tight_layout()\n",
    "f.savefig('plots/accuracy_relative5.pdf', bbox_inches='tight')\n",
    "\n",
    "\n",
    "# # Correlation plot\n",
    "# f, ax = plt.subplots(figsize=(8, 5), ncols=1)\n",
    "# corr = results[['corr_usup', 'corr_isup']]\n",
    "# corr.columns=[r'#$R_{u\\cdot}$', r'#$R_{\\cdot i}$']\n",
    "# # corr.loc['NEG-ITEM-SUPPORT', r'#$R_{\\cdot i}$'] = np.nan\n",
    "# sns.heatmap(corr.round(3), annot=True, vmax=1, vmin=-1, center=0, cmap='vlag')\n",
    "# plt.yticks(fontsize=15)\n",
    "# plt.xticks(rotation=45, fontsize=20)\n",
    "# plt.tight_layout()\n",
    "# plt.savefig('plots/corr5.pdf')\n",
    "\n",
    "# # Ratio\n",
    "# indexs = [index for index in order if index != 'Baseline']\n",
    "# f, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
    "# for index, row in results.iterrows():\n",
    "#     ax[0].plot(row['MAP'].mean(0)[:, -1], lines[index], color=colors[index], label=index, linewidth=3, alpha=0.6)\n",
    "#     ax[1].plot(row['Recall'].mean(0)[:, -1], lines[index], color=colors[index], label=index, linewidth=3, alpha=0.6)\n",
    "# ax[0].set_xticks(np.arange(0, 11))\n",
    "# ax[0].set_xticklabels(np.linspace(0, 1, 11).round(2))\n",
    "# ax[0].set_xlabel(r'$\\lambda$', fontsize=20)\n",
    "# ax[0].set_ylabel('MAP@10', fontsize=20)\n",
    "# ax[1].set_xticks(np.arange(0, 11))\n",
    "# ax[1].set_xticklabels(np.linspace(0, 1, 11).round(2))\n",
    "# ax[1].set_xlabel(r'$\\lambda$', fontsize=20)\n",
    "# ax[1].set_ylabel('Recall@10', fontsize=20)\n",
    "# f.tight_layout()\n",
    "# f.savefig('plots/accuracy_ratio.pdf', bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(nrows=3, ncols=2, figsize=[12, 10])\n",
    "idx = [(0, 0), (0, 1), (1, 0), (1, 1), (2, 0)]\n",
    "model = [aur, icpmf, gaussianMLP, bern_cpmf, 0]\n",
    "label = ['AUR', 'ICPMF', 'Gaussian-MLP', 'HBR-R', 'HBR-Y']\n",
    "for idx, model, label in zip(idx, model, label):\n",
    "    if label == 'HBR-Y':\n",
    "        mu, var = rand_preds[0], rand_preds[1]\n",
    "        constant =  3 / torch.pi**2\n",
    "        denom = np.sqrt(1 + constant * var)\n",
    "        mu_logistic_normal = 1 / (1 + np.exp(-mu / denom))\n",
    "        var_logistic_normal = mu_logistic_normal * (1 - mu_logistic_normal) * (1 - 1 / denom)\n",
    "        rand_preds = [mu_logistic_normal, var_logistic_normal]\n",
    "        ax[idx].set_xlabel(r'Relevance: $E[P_{ui}]$', fontsize=20)\n",
    "        ax[idx].set_ylabel(r'Uncertainty: $Var[P_{ui}]$', fontsize=20)\n",
    "        idx_valid = np.logical_and(rand_preds[1] > -5, rand_preds[1] < 0.1)\n",
    "    else:\n",
    "        rand_preds = model.predict(data.rand['users'][:5000], data.rand['items'][:5000])\n",
    "        ax[idx].set_xlabel(r'Relevance: $\\mu_{ui}$', fontsize=20)\n",
    "        ax[idx].set_ylabel(r'Uncertainty: $\\sigma^2_{ui}$', fontsize=20)\n",
    "        idx_valid = np.logical_and(rand_preds[1] > -5, rand_preds[1] < 5)\n",
    "    if not 'HBR' in label:\n",
    "        rand_preds = rand_preds[0], np.exp(rand_preds[1])\n",
    "    ax[idx].plot(rand_preds[0][idx_valid], rand_preds[1][idx_valid], 'o')\n",
    "    ax[idx].annotate(label, fontsize=20, xy=(0.55, 0.90), xycoords='axes fraction')\n",
    "    b, a = np.polyfit(rand_preds[0][idx_valid], rand_preds[1][idx_valid], deg=1)\n",
    "    xseq = np.linspace(rand_preds[0][idx_valid].min(), rand_preds[0][idx_valid].max(), num=100)\n",
    "    ax[idx].plot(xseq, a + b * xseq, color=\"k\", lw=2.5)\n",
    "\n",
    "\n",
    "f.delaxes(ax[2, 1])\n",
    "f.tight_layout()\n",
    "f.savefig('plots/relevance_uncertainty5.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Not used\n",
    "\n",
    "This is some extra stuff tested during development that were not used in our papers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DoubleMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "name = 'AUR-prior'\n",
    "def init_model(**kwargs):\n",
    "    return JointDoubleMF(data.n_user, data.n_item, embedding_dim=128, embedding_dim_var=128, beta=1/2, ratio=0.9, loss='AUR', **kwargs)\n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    # Parameter setup\n",
    "    params = {'lr': trial.suggest_float('lr', 1e-5, 1e-5, log=True),\n",
    "              'gamma': trial.suggest_float('gamma', 1e-5, 1e-4, log=True),\n",
    "              'weight_decay': trial.suggest_float('wd', 1e-5, 1e-4, log=True),\n",
    "              'n_negatives': trial.suggest_int('neg', 1, 1)}\n",
    "    params_str = '-'.join(f'{key}={value}' for key, value in params.items())\n",
    "    data.batch_size = int(base_batch_size / (params['n_negatives'] + 1))\n",
    "    print(params, data.batch_size)\n",
    "\n",
    "    # Train\n",
    "    model = init_model(**params)\n",
    "    MAP, path, train_likelihood = train(model, data, path='checkpoints/' + name, name=params_str, patience=patience)\n",
    "    trial.set_user_attr('filename', path)\n",
    "    trial.set_user_attr('mean_loss', train_likelihood)\n",
    "\n",
    "    # Rename model file\n",
    "    os.rename('checkpoints/' + name + '/last.ckpt', 'checkpoints/' + name + '/' + params_str + '_last.ckpt')\n",
    "    \n",
    "    return MAP\n",
    "\n",
    "study = run_study(name, objective, n_trials=0)\n",
    "best_runs = study.trials_dataframe().sort_values('value')[::-1][:5]\n",
    "model = init_model().load_from_checkpoint(best_runs.user_attrs_filename.iloc[0])\n",
    "results = test_chap_five(model, data, max_k=10, name=name)\n",
    "\n",
    "clear_output(wait=True)\n",
    "best_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.trials_dataframe().sort_values('value')[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'CPMF'\n",
    "def init_model(**kwargs):\n",
    "    return JointDoubleMF(data.n_user, data.n_item, embedding_dim=128, embedding_dim_var=1, beta=1/2, ratio=0.1, loss='AUR', **kwargs)\n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    # Parameter setup\n",
    "    params = {'lr': trial.suggest_float('lr', 1e-5, 1e-4, log=True),\n",
    "              'gamma': trial.suggest_float('gamma', 1e-3, 1e-1, log=True),\n",
    "              'weight_decay': trial.suggest_float('wd', 1e-6, 1e-3, log=True),\n",
    "              'n_negatives': trial.suggest_int('neg', 1, 50)}\n",
    "    params_str = '-'.join(f'{key}={value}' for key, value in params.items())\n",
    "    data.batch_size = int(base_batch_size / (params['n_negatives'] + 1))\n",
    "    print(params, data.batch_size)\n",
    "\n",
    "    # Train\n",
    "    model = init_model(**params)\n",
    "    MAP, path, train_likelihood = train(model, data, path='checkpoints/' + name, name=params_str, patience=patience)\n",
    "    print(MAP, path, train_likelihood)\n",
    "    trial.set_user_attr('filename', path)\n",
    "    trial.set_user_attr('mean_loss', train_likelihood)\n",
    "\n",
    "    # Rename model file\n",
    "    os.rename('checkpoints/' + name + '/last.ckpt', 'checkpoints/' + name + '/' + params_str + '_last.ckpt')\n",
    "    \n",
    "    return MAP\n",
    "\n",
    "study = run_study(name, objective, n_trials=4)\n",
    "best_runs = study.trials_dataframe().sort_values('value')[::-1][:5]\n",
    "model = init_model().load_from_checkpoint(best_runs.user_attrs_filename.iloc[0])\n",
    "results = test_chap_five(model, data, max_k=10, name=name)\n",
    "\n",
    "clear_output(wait=True)\n",
    "best_runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "name = 'Neural-AUR-large'\n",
    "def init_model(**kwargs):\n",
    "    return GaussianMLP(data.n_user, data.n_item, embedding_dim=128, beta=1/2, ratio=0.1, loss='AUR', **kwargs)\n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    # Parameter setup\n",
    "    params = {'lr': trial.suggest_float('lr', 1e-3, 1e-3, log=True),\n",
    "              'gamma': trial.suggest_float('gamma', 1e-2, 1e-1, log=True),\n",
    "              'dropout': trial.suggest_float('dropout', 0, 0.2),\n",
    "              'n_negatives': trial.suggest_int('neg', 1, 1)}\n",
    "    params_str = '-'.join(f'{key}={value}' for key, value in params.items())\n",
    "    data.batch_size = int(base_batch_size / (params['n_negatives'] + 1))\n",
    "    print(params, data.batch_size)\n",
    "\n",
    "    # Train\n",
    "    model = init_model(**params)\n",
    "    MAP, path, train_likelihood = train(model, data, path='checkpoints/' + name, name=params_str, patience=patience)\n",
    "    trial.set_user_attr('filename', path)\n",
    "    trial.set_user_attr('mean_loss', train_likelihood)\n",
    "    return MAP\n",
    "\n",
    "study = run_study(name, objective, n_trials=1)\n",
    "best_runs = study.trials_dataframe().sort_values('value')[::-1][:5]\n",
    "model = init_model().load_from_checkpoint(best_runs.user_attrs_filename.iloc[0])\n",
    "model.dropl.eval()\n",
    "results = test_chap_five(model, data, max_k=10, name=name)\n",
    "\n",
    "clear_output(wait=True)\n",
    "best_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.trials_dataframe().sort_values('value')[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'AUR-S'\n",
    "def init_model(**kwargs):\n",
    "    return SequentialDoubleMF(baseline=mfmse, embedding_dim_var=128, beta=1/2, loss='AUR', **kwargs)\n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    # Parameter setup\n",
    "    params = {'n_negatives': trial.suggest_int('neg', 20, 50),\n",
    "              'gamma': trial.suggest_float('gamma', 1e-2, 1e-1, log=True)}\n",
    "    params_str = '-'.join(f'{key}={value}' for key, value in params.items())\n",
    "    data.batch_size = int(base_batch_size / (params['n_negatives'] + 1))\n",
    "    print(params, data.batch_size)\n",
    "\n",
    "    # Train\n",
    "    model = init_model(**params)\n",
    "    recall, path, train_likelihood = train(model, data, path='checkpoints/' + name, name=params_str, patience=patience)\n",
    "    trial.set_user_attr('filename', path)\n",
    "    trial.set_user_attr('mean_loss', train_likelihood)\n",
    "    return recall\n",
    "\n",
    "study = run_study(name, objective, n_trials=20)\n",
    "best_runs = study.trials_dataframe().sort_values('value')[::-1]\n",
    "model = init_model().load_from_checkpoint(best_runs.user_attrs_filename.iloc[0])\n",
    "results = test_uncertain(model, data, max_k=10, name=name)\n",
    "\n",
    "clear_output(wait=True)\n",
    "best_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'GBR-S'\n",
    "def init_model(**kwargs):\n",
    "    return SequentialDoubleMF(baseline=mfpoint, embedding_dim_var=128, loss='Pointwise', **kwargs)\n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    # Parameter setup\n",
    "    params = {'n_negatives': trial.suggest_int('neg', 10, 40),\n",
    "              'gamma': trial.suggest_float('gamma', 1e-8, 1e-8, log=True)}\n",
    "    params_str = '-'.join(f'{key}={value}' for key, value in params.items())\n",
    "    data.batch_size = int(base_batch_size / (params['n_negatives'] + 1))\n",
    "    print(params, data.batch_size)\n",
    "\n",
    "    # Train\n",
    "    model = init_model(**params)\n",
    "    recall, path, train_likelihood = train(model, data, path='checkpoints/' + name, name=params_str, patience=patience)\n",
    "    trial.set_user_attr('filename', path)\n",
    "    trial.set_user_attr('mean_loss', train_likelihood)\n",
    "    return recall\n",
    "\n",
    "study = run_study(name, objective, n_trials=2)\n",
    "best_runs = study.trials_dataframe().sort_values('value')[::-1]\n",
    "model = init_model().load_from_checkpoint(best_runs.user_attrs_filename.iloc[0])\n",
    "results = test_uncertain(model, data, max_k=10, name=name)\n",
    "\n",
    "clear_output(wait=True)\n",
    "best_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'GPR-S'\n",
    "def init_model(**kwargs):\n",
    "    return SequentialDoubleMF(baseline=mfpair, embedding_dim_var=128, loss='Pairwise', **kwargs)\n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    # Parameter setup\n",
    "    params = {'gamma': trial.suggest_float('gamma', 1e-6, 1e-2, log=True)}\n",
    "    params_str = '-'.join(f'{key}={value}' for key, value in params.items())\n",
    "    data.batch_size = int(base_batch_size)\n",
    "    print(params, data.batch_size)\n",
    "\n",
    "    # Train\n",
    "    model = init_model(**params)\n",
    "    recall, path, train_likelihood = train(model, data, path='checkpoints/' + name, name=params_str, patience=patience)\n",
    "    trial.set_user_attr('filename', path)\n",
    "    trial.set_user_attr('mean_loss', train_likelihood)\n",
    "    return recall\n",
    "\n",
    "study = run_study(name, objective, n_trials=5)\n",
    "best_runs = study.trials_dataframe().sort_values('value')[::-1]\n",
    "model = init_model().load_from_checkpoint(best_runs.user_attrs_filename.iloc[0])\n",
    "results = test_uncertain(model, data, max_k=10, name=name)\n",
    "\n",
    "clear_output(wait=True)\n",
    "best_runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'AUR-J'\n",
    "def init_model(**kwargs):\n",
    "    return JointDoubleMF(data.n_user, data.n_item, embedding_dim=128, embedding_dim_var=128, loss='AUR', beta=1/2, **kwargs)\n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    # Parameter setup\n",
    "    params = {'weight_decay': trial.suggest_float('wd', 1e-5, 1e-4, log=True),\n",
    "              'n_negatives': trial.suggest_int('neg', 20, 50),\n",
    "              'gamma': trial.suggest_float('gamma', 1e-2, 1e-1, log=True)}\n",
    "    params_str = '-'.join(f'{key}={value}' for key, value in params.items())\n",
    "    data.batch_size = int(base_batch_size / (params['n_negatives'] + 1))\n",
    "    print(params, data.batch_size)\n",
    "\n",
    "    # Train\n",
    "    model = init_model(**params)\n",
    "    recall, path, train_likelihood = train(model, data, path='checkpoints/' + name, name=params_str, patience=patience)\n",
    "    trial.set_user_attr('filename', path)\n",
    "    trial.set_user_attr('mean_loss', train_likelihood)\n",
    "    return recall\n",
    "\n",
    "study = run_study(name, objective, n_trials=1)\n",
    "best_runs = study.trials_dataframe().sort_values('value')[::-1]\n",
    "model = init_model().load_from_checkpoint(best_runs.user_attrs_filename.iloc[0])\n",
    "# results = test_uncertain(model, data, max_k=10, name=name)\n",
    "\n",
    "clear_output(wait=True)\n",
    "best_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'GBR-J'\n",
    "def init_model(**kwargs):\n",
    "    return JointDoubleMF(data.n_user, data.n_item, embedding_dim=128, embedding_dim_var=128, loss='Pointwise', **kwargs)\n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    # Parameter setup\n",
    "    params = {'weight_decay': trial.suggest_float('wd', 1e-5, 1e-2, log=True),\n",
    "              'n_negatives': trial.suggest_int('neg', 10, 40),\n",
    "              'gamma': trial.suggest_float('gamma', 1e-5, 1e-2, log=True)}\n",
    "    params_str = '-'.join(f'{key}={value}' for key, value in params.items())\n",
    "    data.batch_size = int(base_batch_size / (params['n_negatives'] + 1))\n",
    "    print(params, data.batch_size)\n",
    "\n",
    "    # Train\n",
    "    model = init_model(**params)\n",
    "    recall, path, train_likelihood = train(model, data, path='checkpoints/' + name, name=params_str, patience=patience)\n",
    "    trial.set_user_attr('filename', path)\n",
    "    trial.set_user_attr('mean_loss', train_likelihood)\n",
    "    return recall\n",
    "\n",
    "study = run_study(name, objective, n_trials=10)\n",
    "best_runs = study.trials_dataframe().sort_values('value')[::-1]\n",
    "model = init_model().load_from_checkpoint(best_runs.user_attrs_filename.iloc[0])\n",
    "results = test_uncertain(model, data, max_k=10, name=name)\n",
    "\n",
    "clear_output(wait=True)\n",
    "best_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'GPR-J'\n",
    "def init_model(**kwargs):\n",
    "    return JointDoubleMF(data.n_user, data.n_item, embedding_dim=128, embedding_dim_var=128, loss='Pairwise', **kwargs)\n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    # Parameter setup\n",
    "    params = {'weight_decay': trial.suggest_float('wd', 1e-5, 1e-2, log=True),\n",
    "              'gamma': trial.suggest_float('gamma', 1e-6, 1e-2, log=True)}\n",
    "    params_str = '-'.join(f'{key}={value}' for key, value in params.items())\n",
    "    data.batch_size = int(base_batch_size)\n",
    "    print(params, data.batch_size)\n",
    "\n",
    "    # Train\n",
    "    model = init_model(**params)\n",
    "    recall, path, train_likelihood = train(model, data, path='checkpoints/' + name, name=params_str, patience=patience)\n",
    "    trial.set_user_attr('filename', path)\n",
    "    trial.set_user_attr('mean_loss', train_likelihood)\n",
    "    return recall\n",
    "\n",
    "study = run_study(name, objective, n_trials=5)\n",
    "best_runs = study.trials_dataframe().sort_values('value')[::-1]\n",
    "model = init_model().load_from_checkpoint(best_runs.user_attrs_filename.iloc[0])\n",
    "results = test_uncertain(model, data, max_k=10, name=name)\n",
    "\n",
    "clear_output(wait=True)\n",
    "best_runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results\n",
    "results = {}\n",
    "for key in os.listdir('results'):\n",
    "    results[key.replace('.pkl', '').replace('_', ' ')] = pickle.load(open(os.path.join('results', key), 'rb'))\n",
    "order = ['MF-BPR', 'MF-BCE', 'MF-Pointwise', 'MF-Pairwise', 'MF-MSE', \n",
    "         'AUR-J', 'GPR-J', 'GBR-J',\n",
    "         'AUR-S', 'GPR-S', 'GBR-S']\n",
    "drop = ['MF-BPR', 'MF-BCE', 'MF-Pointwise', 'MF-Pairwise', 'MF-MSE']\n",
    "results = pd.DataFrame([results[key] for key in order], index=order)\n",
    "\n",
    "# Plot aestetics\n",
    "colors = [c for c in list(TABLEAU_COLORS)] + ['k', 'b', 'g', 'r', 'm']\n",
    "colors = {k:c for k, c in zip(results.index, colors)}\n",
    "lines = ['o', 'v', '^', '<', '>', 's', 'p', '+', 'x', '*', 'p']\n",
    "lines = {k: '-' + l for k, l, in zip(results.index, lines)}\n",
    "\n",
    "# Results\n",
    "results = pd.DataFrame([results[key] for key in order], index=order)\n",
    "results[['FCP', 'URI', 'UAC']].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation plot\n",
    "corr = results.drop(index=drop)[['corr_usup', 'corr_isup']]\n",
    "corr.columns=[r'#$R_{u\\cdot}$', r'#$R_{\\cdot i}$']\n",
    "sns.heatmap(corr.round(3), annot=True, vmax=1, vmin=-1, center=0, cmap='vlag')\n",
    "plt.xticks(rotation=45, fontsize=15)\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/corr.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Top-K accuracy metrics\n",
    "f, ax = plt.subplots(figsize=(10, 5), ncols=2)\n",
    "for index, row in results.drop(index=['MF-BPR', 'MF-MSE', 'MF-Pointwise', 'MF-Pairwise']).iterrows():\n",
    "    try:\n",
    "        ax[0].plot(np.arange(1, 11), row['MAP2'], lines[index], color=colors[index], label=index, linewidth=3, alpha=0.6)\n",
    "        ax[1].plot(np.arange(1, 11), row['Recall2'], lines[index], color=colors[index], label=index, linewidth=3, alpha=0.6)\n",
    "    except:\n",
    "        ax[0].plot(np.arange(1, 11), row['MAP'], lines[index], color=colors[index], label=index, linewidth=3, alpha=0.6)\n",
    "        ax[1].plot(np.arange(1, 11), row['Recall'], lines[index], color=colors[index], label=index, linewidth=3, alpha=0.6)\n",
    "ax[0].set_xticks(np.arange(1, 11))\n",
    "ax[0].set_xlabel('n', fontsize=20)\n",
    "ax[0].set_ylabel('MAP@n', fontsize=20)\n",
    "ax[1].set_xticks(np.arange(1, 11))\n",
    "ax[1].set_xlabel('n', fontsize=20)\n",
    "ax[1].set_ylabel('Recall@n', fontsize=20)\n",
    "handles, labels = ax[0].get_legend_handles_labels()\n",
    "f.legend(handles, labels, fontsize=15, ncol=4, bbox_to_anchor=(0.86, 1.12))\n",
    "f.tight_layout()\n",
    "f.savefig('plots/accuracy.pdf')\n",
    "results.drop(index=['MF-BPR', 'MF-BCE'])[['MAP', 'MAP2', 'Recall', 'Recall2']].transform(lambda x: x.str[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MAP vs Uncertainty and vs UserUnceratinty\n",
    "f, ax = plt.subplots(figsize=(6, 5))\n",
    "x = np.arange(10) + 1\n",
    "for index, row in results.drop(index=drop).iterrows():\n",
    "    ax.plot(x, row['MAP-Uncertainty'], lines[index], color=colors[index], label=index, linewidth=3, alpha=0.6)\n",
    "ax.set(xticks = x)\n",
    "ax.set_xlabel('User uncertainty bin', fontsize=25)\n",
    "ax.set_ylabel('MAP@10', fontsize=20)\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "f.legend(handles, labels, fontsize=15, ncol=3, bbox_to_anchor=(0.94, 1.10))\n",
    "f.tight_layout()\n",
    "f.savefig('plots/MAP-Uncertainty.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## MAP and Uncertainty vs Profile size\n",
    "f, ax = plt.subplots(figsize=(6, 5))\n",
    "for index, row in results.drop(index=drop).iterrows():\n",
    "    ax.plot(x, row['MAP-ProfSize'], lines[index], color=colors[index], label=index, linewidth=3, alpha=0.6)\n",
    "ax.set(xticks = x)\n",
    "ax.set_xlabel('Profile size bin', fontsize=25)\n",
    "ax.set_ylabel('MAP@10', fontsize=20)\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "f.legend(handles, labels, fontsize=15, ncol=3, bbox_to_anchor=(0.94, 1.10))\n",
    "f.tight_layout()\n",
    "f.savefig('plots/MAP-ProfSize.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'MLP-GBR'\n",
    "def init_model(**kwargs):\n",
    "    return GaussianMLP(data.n_user, data.n_item, embedding_dim=128, loss='Pointwise', **kwargs)\n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    # Parameter setup\n",
    "    params = {'dropout': trial.suggest_float('dropout', 0, 0.2),\n",
    "              'n_negatives': trial.suggest_int('neg', 5, 50)}\n",
    "    params_str = '-'.join(f'{key}={value}' for key, value in params.items())\n",
    "    data.batch_size = int(base_batch_size / (params['n_negatives'] + 1))\n",
    "    print(params, data.batch_size)\n",
    "\n",
    "    # Train\n",
    "    model = init_model(**params)\n",
    "    MAP, path, train_likelihood = train(model, data, path='checkpoints/' + name, name=params_str, patience=patience)\n",
    "    trial.set_user_attr('filename', path)\n",
    "    trial.set_user_attr('mean_loss', train_likelihood)\n",
    "    return MAP\n",
    "\n",
    "study = run_study(name, objective, n_trials=5)\n",
    "best_runs = study.trials_dataframe().sort_values('value')[::-1]\n",
    "model = init_model().load_from_checkpoint(best_runs.user_attrs_filename.iloc[0])\n",
    "results = test_vanilla(model, data, max_k=10, name=name)\n",
    "\n",
    "clear_output(wait=True)\n",
    "print(results['MAP'])\n",
    "best_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'MLP-GBR'\n",
    "def init_model(**kwargs):\n",
    "    return GaussianMLP(data.n_user, data.n_item, embedding_dim=128, loss='Pointwise', **kwargs)\n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    # Parameter setup\n",
    "    params = {'dropout': trial.suggest_float('dropout', 0, 0.2),\n",
    "              'n_negatives': trial.suggest_int('neg', 5, 50)}\n",
    "    params_str = '-'.join(f'{key}={value}' for key, value in params.items())\n",
    "    data.batch_size = int(base_batch_size / (params['n_negatives'] + 1))\n",
    "    print(params, data.batch_size)\n",
    "\n",
    "    # Train\n",
    "    model = init_model(**params)\n",
    "    MAP, path, train_likelihood = train(model, data, path='checkpoints/' + name, name=params_str, patience=patience)\n",
    "    trial.set_user_attr('filename', path)\n",
    "    trial.set_user_attr('mean_loss', train_likelihood)\n",
    "    return MAP\n",
    "\n",
    "study = run_study(name, objective, n_trials=5)\n",
    "best_runs = study.trials_dataframe().sort_values('value')[::-1]\n",
    "model = init_model().load_from_checkpoint(best_runs.user_attrs_filename.iloc[0])\n",
    "results = test_vanilla(model, data, max_k=10, name=name)\n",
    "\n",
    "clear_output(wait=True)\n",
    "print(results['MAP'])\n",
    "best_runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'MLP'\n",
    "def init_model(**kwargs):\n",
    "    return MLP(data.n_user, data.n_item, **kwargs)\n",
    "base_conf = {'embedding_dim': 10, 'lr': 0, 'n_negatives': 0}\n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    # Parameter setup\n",
    "    batch_size = trial.suggest_int('bs', 256, 256)\n",
    "    params = {'embedding_dim': trial.suggest_int('dim', 64, 64),\n",
    "              'lr': trial.suggest_float('lr', 1e-4, 1e-2),\n",
    "              'n_negatives': trial.suggest_int('neg', 2, 20),\n",
    "              'dropout': trial.suggest_float('layers', 0, 0.2)}\n",
    "    params_str = f'bs = {batch_size}-' + '-'.join(f'{key}={value}' for key, value in params.items())\n",
    "\n",
    "    # Train\n",
    "    data.batch_size = batch_size\n",
    "    model = init_model(**params)\n",
    "    MAP, path = train(model, data, path='checkpoints/' + name, name=params_str)\n",
    "    trial.set_user_attr('filename', path)\n",
    "    return MAP\n",
    "\n",
    "study = run_study(name, objective, n_trials=trials)\n",
    "mlp, runs = load(init_model(**base_conf), study)\n",
    "# results = test_vanilla(mlp, data, max_k=10, name=name)\n",
    "\n",
    "clear_output(wait=True)\n",
    "# print(results)\n",
    "runs[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results\n",
    "results = {}\n",
    "for key in os.listdir('results'):\n",
    "    results[key.replace('.pkl', '').replace('_', ' ')] = pickle.load(open(os.path.join('results', key), 'rb'))\n",
    "order = ['MF-BPR', 'MF-BCE', 'MF-Pointwise', 'MF-Pairwise', 'MF-MSE', \n",
    "         'AUR', 'Pointwise', 'Pairwise']\n",
    "drop = ['MF-BPR', 'MF-BCE', 'MF-Pointwise', 'MF-Pairwise', 'MF-MSE']\n",
    "results = pd.DataFrame([results[key] for key in order], index=order)\n",
    "\n",
    "# Plot aestetics\n",
    "colors = [c for c in list(TABLEAU_COLORS)] + ['k', 'b', 'g', 'r', 'm']\n",
    "colors = {k:c for k, c in zip(results.index, colors)}\n",
    "lines = ['o', 'v', '^', '<', '>', 's', 'p', '+', 'x', '*', 'p']\n",
    "lines = {k: '-' + l for k, l, in zip(results.index, lines)}\n",
    "\n",
    "# Results\n",
    "results[['FCP', 'URI', 'UAC']].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation plot\n",
    "corr = results.drop(index=drop)[['corr_usup', 'corr_isup']]\n",
    "corr.columns=[r'#$R_{u\\cdot}$', r'#$R_{\\cdot i}$']\n",
    "sns.heatmap(corr.round(3), annot=True, vmax=1, vmin=-1, center=0, cmap='vlag')\n",
    "plt.xticks(rotation=45, fontsize=15)\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/corr.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Top-K accuracy metrics\n",
    "f, ax = plt.subplots(figsize=(10, 5), ncols=2)\n",
    "for index, row in results.iterrows():\n",
    "    try:\n",
    "        ax[0].plot(np.arange(1, 11), row['MAP2'], lines[index], color=colors[index], label=index, linewidth=3, alpha=0.6)\n",
    "        ax[1].plot(np.arange(1, 11), row['Recall2'], lines[index], color=colors[index], label=index, linewidth=3, alpha=0.6)\n",
    "    except:\n",
    "        ax[0].plot(np.arange(1, 11), row['MAP'], lines[index], color=colors[index], label=index, linewidth=3, alpha=0.6)\n",
    "        ax[1].plot(np.arange(1, 11), row['Recall'], lines[index], color=colors[index], label=index, linewidth=3, alpha=0.6)\n",
    "ax[0].set_xticks(np.arange(1, 11))\n",
    "ax[0].set_xlabel('n', fontsize=20)\n",
    "ax[0].set_ylabel('MAP@n', fontsize=20)\n",
    "ax[1].set_xticks(np.arange(1, 11))\n",
    "ax[1].set_xlabel('n', fontsize=20)\n",
    "ax[1].set_ylabel('Recall@n', fontsize=20)\n",
    "handles, labels = ax[0].get_legend_handles_labels()\n",
    "f.legend(handles, labels, fontsize=15, ncol=3, bbox_to_anchor=(0.9, 1.3))\n",
    "f.tight_layout()\n",
    "f.savefig('plots/accuracy.pdf', bbox_inches='tight')\n",
    "\n",
    "table = results.drop(index=drop)[['MAP', 'MAP2', 'Recall', 'Recall2']].transform(lambda x: x.str[-1])\n",
    "table.columns = ['MAP', 'UA MAP', 'Recall', 'UA Recall']\n",
    "table['MAP-Improvement'] = (((table['UA MAP'] / table['MAP']) - 1) * 100).round(3).astype(str).add('%')\n",
    "table['Recall-Improvement'] = (((table['UA Recall'] / table['Recall']) - 1) * 100).round(3).astype(str).add('%')\n",
    "table.to_csv('plots/improvements.csv')\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results\n",
    "results = {}\n",
    "for key in os.listdir('results'):\n",
    "    results[key.replace('.pkl', '').replace('_', ' ')] = pickle.load(open(os.path.join('results', key), 'rb'))\n",
    "order = ['MF', 'MF-ENSEMBLE', 'MLP', 'MLP-ENSEMBLE', 'MCDropout', 'BayesianMLP', 'MF-NIS' , 'MLP-NIS', 'MF-NUS' , 'MLP-NUS']\n",
    "results = pd.DataFrame([results[key] for key in order], index=order)\n",
    "\n",
    "# Plot aestetics\n",
    "colors = [c for c in list(TABLEAU_COLORS)] + ['k', 'b', 'g', 'r']\n",
    "colors = {k:c for k, c in zip(results.index, colors)}\n",
    "lines = ['o', 'v', '^', '<', '>', 's', 'p', '+', 'x', '*']\n",
    "lines = {k: '-' + l for k, l, in zip(results, lines)}\n",
    "\n",
    "# Results\n",
    "results[['FCP', 'URI', 'UAC']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Top-K accuracy metrics\n",
    "f, ax = plt.subplots(figsize=(10, 5), ncols=2)\n",
    "for index, row in results.drop(['MF-NIS', 'MF-NUS', 'MLP-NUS', 'MLP-NIS']).iterrows():\n",
    "    ax[0].plot(np.arange(1, 11), row['MAP'], '-', color=colors[index], label=index, linewidth=5, alpha=0.6)\n",
    "    ax[1].plot(np.arange(1, 11), row['Recall'], '-', color=colors[index], label=index, linewidth=5, alpha=0.6)\n",
    "ax[0].set_xticks(np.arange(1, 11))\n",
    "ax[0].set_xlabel('n', fontsize=20)\n",
    "ax[0].set_ylabel('MAP@n', fontsize=20)\n",
    "ax[1].set_xticks(np.arange(1, 11))\n",
    "ax[1].set_xlabel('n', fontsize=20)\n",
    "ax[1].set_ylabel('Recall@n', fontsize=20)\n",
    "handles, labels = ax[0].get_legend_handles_labels()\n",
    "f.legend(handles, labels, fontsize=15, ncol=3, bbox_to_anchor=(0.85, 1.1))\n",
    "f.tight_layout()\n",
    "f.savefig('plots/accuracy.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MAP vs Uncertainty\n",
    "f, ax = plt.subplots(figsize=(6, 5))\n",
    "x = np.arange(10) + 1\n",
    "for index, row in results.drop(['MF', 'MLP']).iterrows():\n",
    "    ax.plot(x, row['MAP-Uncertainty'], '-', color=colors[index], label=index, linewidth=5, alpha=0.6)\n",
    "ax.set(xticks = x)\n",
    "ax.set_xlabel('Uncertainty bin', fontsize=25)\n",
    "ax.set_ylabel('MAP@10', fontsize=20)\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "f.legend(handles, labels, fontsize=15, ncol=3, bbox_to_anchor=(0.85, 1.1))\n",
    "f.tight_layout()\n",
    "f.savefig('plots/MAP-Uncertainty.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MAP vs Coverage\n",
    "f, ax = plt.subplots(figsize=(6, 5))\n",
    "x = np.linspace(-3, 3, 11)\n",
    "for index, row in results.drop(['MF', 'MLP']).iterrows():\n",
    "    ax.plot(x, row['unc_MAP'], '-', color=colors[index], label=index, linewidth=5, alpha=0.6)\n",
    "ax.set(xticks = x, xticklabels = [str(round(x, 1)) for x in x])\n",
    "ax.set_xlabel(r'$\\lambda$', fontsize=25)\n",
    "ax.set_ylabel('MAP@10', fontsize=20)\n",
    "ax.legend(fontsize=15)\n",
    "f.tight_layout()\n",
    "f.savefig('plots/accuracy_lambda.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = ['MCDropout', 'BayesianMLP']\n",
    "f, ax = plt.subplots(figsize=(12, 5), ncols=len(idx))\n",
    "for i, model in enumerate(idx):\n",
    "    ax[i].hist(results.loc[model]['norm_unc'][0], bins='auto', alpha=0.6, density=True, label='Hits')\n",
    "    ax[i].hist(results.loc[model]['norm_unc'][1], bins='auto', alpha=0.4, density=True, label='Non-hits')\n",
    "    ax[i].legend(fontsize=15)\n",
    "    ax[i].set_xlabel('Normalized uncertainty', fontsize=20)\n",
    "    ax[i].set_ylabel('Density', fontsize=20)\n",
    "    ax[i].set_title(model)\n",
    "f.tight_layout()\n",
    "f.savefig('plots/normalized_uncertainty.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
