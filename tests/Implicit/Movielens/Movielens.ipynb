{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from copy import deepcopy\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from matplotlib.colors import TABLEAU_COLORS\n",
    "\n",
    "sys.path.append('../../..')\n",
    "from uncertain.utils.data import Data\n",
    "from uncertain.utils.training import train, run_study, load\n",
    "from uncertain.utils.evaluation import test_vanilla, test_uncertain, test_chap_five\n",
    "\n",
    "from uncertain.implicit.base import MF, MLP\n",
    "from uncertain.implicit.DoubleMF import JointDoubleMF, SequentialDoubleMF\n",
    "from uncertain.implicit.extras import HeuristicUncertainty\n",
    "from uncertain.implicit.neural import GaussianMLP, MCDropout, Ensemble\n",
    "\n",
    "if os.path.isfile('data.pkl'):\n",
    "    with open('data.pkl', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    print(f'Data prepared: {data.n_user} users, {data.n_item} items.')\n",
    "    print(f'{len(data.train)} train, {len(data.val)} validation and {len(data.test)} test interactions.')\n",
    "    data.batch_size = int(1e3)\n",
    "else:\n",
    "    data = pd.read_table('data.csv', sep=',', header=0)\n",
    "    data.columns = ['user', 'item', 'score', 'timestamps']\n",
    "    # data = data.drop(columns='timestamps') # Trying randomly shuffled data\n",
    "    data = Data(data, implicit=True)\n",
    "    with open('data.pkl', 'wb') as f:\n",
    "        pickle.dump(data, f, protocol=5)\n",
    "\n",
    "data.item_support = data.item_support.astype(float)\n",
    "data.user_support = data.user_support.astype(float)        \n",
    "\n",
    "base_batch_size = 1024 *5\n",
    "trials = 0 ## 0 for eval only mode\n",
    "patience = 2 ## Number of validation checks before ending training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the tail items accordind to AUR paper\n",
    "import torch\n",
    "\n",
    "n_inter = data.item_support.sum()\n",
    "order = np.argsort(data.item_support)\n",
    "\n",
    "for n_tail in range(data.n_item):\n",
    "    ratio_tail = np.sum(data.item_support[order[:n_tail]]) / n_inter\n",
    "    if ratio_tail > 0.5:\n",
    "        print(n_tail)\n",
    "        break\n",
    "\n",
    "data.tail_items = torch.tensor(order[:n_tail])\n",
    "data.head_items = torch.tensor(order[n_tail:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "name = 'MF-BCE'\n",
    "def init_model(**kwargs):\n",
    "    return MF(data.n_user, data.n_item, embedding_dim=128, loss='BCE', **kwargs)\n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    # Parameter setup\n",
    "    params = {'lr': trial.suggest_float('lr', 1e-4, 1e-2, log=True),\n",
    "              'weight_decay': trial.suggest_float('wd', 1e-5, 1e-2, log=True),\n",
    "              'n_negatives': trial.suggest_int('neg', 1, 50)}\n",
    "    params_str = '-'.join(f'{key}={value}' for key, value in params.items())\n",
    "    data.batch_size = int(base_batch_size / (params['n_negatives'] + 1))\n",
    "    print(params, data.batch_size)\n",
    "\n",
    "    # Train\n",
    "    model = init_model(**params)\n",
    "    MAP, path, train_likelihood = train(model, data, path='checkpoints/' + name, name=params_str, patience=patience)\n",
    "    trial.set_user_attr('filename', path)\n",
    "    trial.set_user_attr('mean_loss', train_likelihood)\n",
    "    return MAP\n",
    "\n",
    "study = run_study(name, objective, n_trials=trials)\n",
    "best_runs = study.trials_dataframe().sort_values('value')[::-1][:5]\n",
    "mfbce = init_model().load_from_checkpoint(best_runs.user_attrs_filename.iloc[0])\n",
    "\n",
    "results = test_vanilla(mfbce, data, max_k=10, name=name)\n",
    "clear_output(wait=True)\n",
    "print(results['MAP'])\n",
    "best_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'MF-BPR'\n",
    "def init_model(**kwargs):\n",
    "    return MF(data.n_user, data.n_item, embedding_dim=128, loss='BPR', **kwargs)\n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    # Parameter setup\n",
    "    params = {'lr': trial.suggest_float('lr', 1e-4, 1e-2, log=True),\n",
    "              'weight_decay': trial.suggest_float('wd', 1e-5, 1e-2, log=True)}\n",
    "    params_str = '-'.join(f'{key}={value}' for key, value in params.items())\n",
    "    data.batch_size = int(base_batch_size / 2)\n",
    "    print(params, data.batch_size)\n",
    "\n",
    "    # Train\n",
    "    model = init_model(**params)\n",
    "    MAP, path, train_likelihood = train(model, data, path='checkpoints/' + name, name=params_str, patience=patience)\n",
    "    trial.set_user_attr('filename', path)\n",
    "    trial.set_user_attr('mean_loss', train_likelihood)\n",
    "    return MAP\n",
    "\n",
    "study = run_study(name, objective, n_trials=0)\n",
    "best_runs = study.trials_dataframe().sort_values('value')[::-1]\n",
    "mfbpr = init_model().load_from_checkpoint(best_runs.user_attrs_filename.iloc[0])\n",
    "\n",
    "results = test_vanilla(mfbpr, data, max_k=10, name=name)\n",
    "# clear_output(wait=True)\n",
    "print(results['MAP'])\n",
    "best_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'MF-MSE'\n",
    "def init_model(**kwargs):\n",
    "    return MF(data.n_user, data.n_item, embedding_dim=128, loss='MSE', **kwargs)\n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    # Parameter setup\n",
    "    params = {'lr': trial.suggest_float('lr', 1e-4, 1e-2, log=True),\n",
    "              'weight_decay': trial.suggest_float('wd', 1e-5, 1e-2, log=True),\n",
    "              'n_negatives': trial.suggest_int('neg', 1, 50)}\n",
    "    params_str = '-'.join(f'{key}={value}' for key, value in params.items())\n",
    "    data.batch_size = int(base_batch_size / (params['n_negatives'] + 1))\n",
    "    print(params, data.batch_size)\n",
    "\n",
    "    # Train\n",
    "    model = init_model(**params)\n",
    "    MAP, path, train_likelihood = train(model, data, path='checkpoints/' + name, name=params_str, patience=patience)\n",
    "    trial.set_user_attr('filename', path)\n",
    "    trial.set_user_attr('mean_loss', train_likelihood)\n",
    "    return MAP\n",
    "\n",
    "study = run_study(name, objective, n_trials=trials)\n",
    "best_runs = study.trials_dataframe().sort_values('value')[::-1]\n",
    "mfmse = init_model().load_from_checkpoint(best_runs.user_attrs_filename.iloc[0])\n",
    "\n",
    "results = test_vanilla(mfmse, data, max_k=10, name=name)\n",
    "clear_output(wait=True)\n",
    "print(results['MAP'])\n",
    "best_runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'MLP-BCE'\n",
    "def init_model(**kwargs):\n",
    "    return MLP(data.n_user, data.n_item, embedding_dim=128, loss='BCE', **kwargs)\n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    # Parameter setup\n",
    "    params = {'lr': trial.suggest_float('lr', 1e-4, 1e-2, log=True),\n",
    "              'dropout': trial.suggest_float('dropout', 0, 0.2),\n",
    "              'n_negatives': trial.suggest_int('neg', 1, 50)}\n",
    "    params_str = '-'.join(f'{key}={value}' for key, value in params.items())\n",
    "    data.batch_size = int(base_batch_size / (params['n_negatives'] + 1))\n",
    "    print(params, data.batch_size)\n",
    "\n",
    "    # Train\n",
    "    model = init_model(**params)\n",
    "    MAP, path, train_likelihood = train(model, data, path='checkpoints/' + name, name=params_str, patience=patience)\n",
    "    trial.set_user_attr('filename', path)\n",
    "    trial.set_user_attr('mean_loss', train_likelihood)\n",
    "    return MAP\n",
    "\n",
    "study = run_study(name, objective, n_trials=0)\n",
    "best_runs = study.trials_dataframe().sort_values('value')[::-1]\n",
    "mlpbce = init_model().load_from_checkpoint(best_runs.user_attrs_filename.iloc[0])\n",
    "'''\n",
    "results = test_vanilla(mlpbce, data, max_k=10, name=name)\n",
    "clear_output(wait=True)\n",
    "print(results['MAP'])\n",
    "best_runs\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'MLP-BPR'\n",
    "def init_model(**kwargs):\n",
    "    return MLP(data.n_user, data.n_item, embedding_dim=128, loss='BPR', **kwargs)\n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    # Parameter setup\n",
    "    params = {'lr': trial.suggest_float('lr', 1e-4, 1e-2, log=True),\n",
    "              'dropout': trial.suggest_float('dropout', 0, 0.2)}\n",
    "    params_str = '-'.join(f'{key}={value}' for key, value in params.items())\n",
    "    data.batch_size = int(base_batch_size / 2)\n",
    "    print(params, data.batch_size)\n",
    "\n",
    "    # Train\n",
    "    model = init_model(**params)\n",
    "    MAP, path, train_likelihood = train(model, data, path='checkpoints/' + name, name=params_str, patience=patience)\n",
    "    trial.set_user_attr('filename', path)\n",
    "    trial.set_user_attr('mean_loss', train_likelihood)\n",
    "    return MAP\n",
    "\n",
    "study = run_study(name, objective, n_trials=trials)\n",
    "best_runs = study.trials_dataframe().sort_values('value')[::-1]\n",
    "mlpbpr = init_model().load_from_checkpoint(best_runs.user_attrs_filename.iloc[0])\n",
    "\n",
    "results = test_vanilla(mlpbpr, data, max_k=10, name=name)\n",
    "clear_output(wait=True)\n",
    "print(results['MAP'])\n",
    "best_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'MLP-MSE'\n",
    "def init_model(**kwargs):\n",
    "    return MLP(data.n_user, data.n_item, loss='MSE', **kwargs)\n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    # Parameter setup\n",
    "    params = {'lr': trial.suggest_float('lr', 1e-4, 1e-2, log=True),\n",
    "              'dropout': trial.suggest_float('dropout', 0, 0.2),\n",
    "              'n_negatives': trial.suggest_int('neg', 1, 50)}\n",
    "    params_str = '-'.join(f'{key}={value}' for key, value in params.items())\n",
    "    data.batch_size = int(base_batch_size / (params['n_negatives'] + 1))\n",
    "    print(params, data.batch_size)\n",
    "\n",
    "    # Train\n",
    "    model = init_model(**params)\n",
    "    MAP, path, train_likelihood = train(model, data, path='checkpoints/' + name, name=params_str, patience=patience)\n",
    "    trial.set_user_attr('filename', path)\n",
    "    trial.set_user_attr('mean_loss', train_likelihood)\n",
    "    return MAP\n",
    "\n",
    "study = run_study(name, objective, n_trials=trials)\n",
    "best_runs = study.trials_dataframe().sort_values('value')[::-1]\n",
    "model = init_model().load_from_checkpoint(best_runs.user_attrs_filename.iloc[0])\n",
    "\n",
    "results = test_vanilla(model, data, max_k=10, name=name)\n",
    "clear_output(wait=True)\n",
    "print(results['MAP'])\n",
    "best_runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load results\n",
    "results = {}\n",
    "for key in os.listdir('results'):\n",
    "    results[key.replace('.pkl', '').replace('_', ' ')] = pickle.load(open(os.path.join('results', key), 'rb'))\n",
    "order = ['MF-MSE', 'MF-BCE', 'MF-BPR',\n",
    "         'MLP-MSE', 'MLP-BCE', 'MLP-BPR']\n",
    "results = pd.DataFrame([results[key] for key in order], index=order)\n",
    "\n",
    "# Plot aestetics\n",
    "colors = [c for c in list(TABLEAU_COLORS)]\n",
    "colors = {k:c for k, c in zip(results.index, colors)}\n",
    "lines = ['o', 'v', '^', '<', '>', 's', 'p', '+', 'x', '*', 'p']\n",
    "lines = {k: '-' + l for k, l, in zip(results.index, lines)}\n",
    "\n",
    "# Results\n",
    "print(results['FCP'])\n",
    "\n",
    "## Top-K accuracy metrics\n",
    "f, ax = plt.subplots(figsize=(10, 5), ncols=2)\n",
    "for index, row in results.iterrows():\n",
    "    ax[0].plot(np.arange(1, 11), row['MAP'], lines[index], color=colors[index], label=index, linewidth=3, alpha=0.6)\n",
    "    ax[1].plot(np.arange(1, 11), row['Recall'], lines[index], color=colors[index], label=index, linewidth=3, alpha=0.6)\n",
    "ax[0].set_xticks(np.arange(1, 11))\n",
    "ax[0].set_xlabel('n', fontsize=20)\n",
    "ax[0].set_ylabel('MAP@n', fontsize=20)\n",
    "ax[1].set_xticks(np.arange(1, 11))\n",
    "ax[1].set_xlabel('n', fontsize=20)\n",
    "ax[1].set_ylabel('Recall@n', fontsize=20)\n",
    "handles, labels = ax[0].get_legend_handles_labels()\n",
    "f.legend(handles, labels, fontsize=15, ncol=3, bbox_to_anchor=(0.83, 1.15))\n",
    "f.tight_layout()\n",
    "f.savefig('plots/baselines.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User / Item support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = test_uncertain(HeuristicUncertainty(baseline=mfbpr, user_uncertainty=-data.user_support), data, name='NUS', max_k=10)\n",
    "print(results['MAP-Uncertainty'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "isup = HeuristicUncertainty(baseline=mfbpr, item_uncertainty=-data.item_support)\n",
    "results = test_uncertain(isup, data, name='NIS', max_k=10)\n",
    "print(results['MAP-Uncertainty'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HeuristicUncertainty(baseline=mfbpr, item_uncertainty=-data.item_support)\n",
    "model.recommend(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load params\n",
    "best_params = study.best_params\n",
    "data.batch_size = int(base_batch_size / 2)\n",
    "best_params = {'weight_decay': best_params['wd']}\n",
    "\n",
    "# Train\n",
    "if trials > 0:\n",
    "    for i in range(4):\n",
    "        model = init_model(**best_params)\n",
    "        train(model, data, path='checkpoints/ensemble_MF', name=f'{i}')\n",
    "\n",
    "# Load ensemble models\n",
    "models = [mfbpr]\n",
    "for file in os.listdir('checkpoints/ensemble_MF'):\n",
    "    models.append(init_model())\n",
    "    models[-1] = models[-1].load_from_checkpoint(os.path.join('checkpoints/ensemble_MF', file))\n",
    "    models[-1].eval()\n",
    "ensemble_mf = Ensemble(models)\n",
    "clear_output(wait=True)\n",
    "results = test_uncertain(ensemble_mf, data, name='MF-ENSEMBLE', max_k=10)\n",
    "print(results['MAP'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MC Dropout\n",
    "dropout = MCDropout(base_model=mlpbce, mc_iteration=5)\n",
    "results = test_uncertain(dropout, data, max_k=10, name='MCDropout')\n",
    "print(results['MAP-Uncertainty'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BayesianNN\n",
    "name = 'BayesianMLP'\n",
    "def init_model(**kwargs):\n",
    "    return BayesianMLP(data.n_user, data.n_item, **kwargs)\n",
    "base_conf = {'embedding_dim': 10, 'lr': 0, 'n_negatives': 0, 'num_batches': int(len(data.train) / 256)}\n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    # Parameter setup\n",
    "    batch_size = trial.suggest_int('bs', 256, 256)\n",
    "    params = {'embedding_dim': trial.suggest_int('dim', 128, 128),\n",
    "              'lr': trial.suggest_float('lr', 1e-4, 1e-2),\n",
    "              'n_negatives': trial.suggest_int('neg', 20, 20),\n",
    "              'sample_train': trial.suggest_int('train_samples', 1, 10),\n",
    "              'prior_pi': trial.suggest_categorical('pi', [1/4, 1/2, 3/4]),\n",
    "              'prior_sigma_1': trial.suggest_categorical('sigma1', np.exp(-np.array([0, 1, 2]))),\n",
    "              'prior_sigma_2': trial.suggest_categorical('sigma2', np.exp(-np.array([6, 7, 8])))}\n",
    "    params['num_batches'] = int(len(data.train) / 256)\n",
    "    params_str = f'bs = {batch_size}-' + '-'.join(f'{key}={value}' for key, value in params.items())\n",
    "\n",
    "    # Train\n",
    "    data.batch_size = batch_size\n",
    "    model = init_model(**params)\n",
    "    MAP, path = train(model, data, path='checkpoints/' + name, name=params_str)\n",
    "    trial.set_user_attr('filename', path)\n",
    "    return MAP\n",
    "\n",
    "study = run_study(name, objective, n_trials=5)\n",
    "BayesMLP, runs = load(init_model(**base_conf), study, top=0)\n",
    "results = test_uncertain(BayesMLP, data, max_k=10, name=name)\n",
    "\n",
    "clear_output(wait=True)\n",
    "print(results['MAP'])\n",
    "runs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Deep Ensemble\n",
    "# Load params\n",
    "best_params = study.best_params\n",
    "data.batch_size = int(base_batch_size / 2)\n",
    "best_params = {'dropout': best_params['dropout']}\n",
    "\n",
    "# Train\n",
    "if trials > 0:\n",
    "    for i in range(4):\n",
    "        model = init_model(**best_params)\n",
    "        train(model, data, path='checkpoints/ensemble_MLP', name=f'{i}')\n",
    "\n",
    "# Load ensemble models\n",
    "models = [mlpbce]\n",
    "for file in os.listdir('checkpoints/ensemble_MLP'):\n",
    "    models.append(MLP(data.n_user, data.n_item))\n",
    "    models[-1] = models[-1].load_from_checkpoint(os.path.join('checkpoints/ensemble_MLP', file))\n",
    "    models[-1].eval()\n",
    "ensemble_mlp = Ensemble(models)\n",
    "clear_output(wait=True)\n",
    "results = test_uncertain(ensemble_mlp, data, name='MLP-ENSEMBLE', max_k=10)\n",
    "print(results['MAP-Uncertainty'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline results\n",
    "results = {}\n",
    "for key in os.listdir('results'):\n",
    "    results[key.replace('.pkl', '').replace('_', ' ')] = pickle.load(open(os.path.join('results', key), 'rb'))\n",
    "order = ['MF-MSE', 'MF-BCE', 'MF-BPR', 'MLP-MSE', 'MLP-BCE', 'MLP-BPR']\n",
    "results = pd.DataFrame([results[key] for key in order], index=order)\n",
    "\n",
    "# Plot aestetics\n",
    "colors = [c for c in list(TABLEAU_COLORS)]\n",
    "colors = {k:c for k, c in zip(results.index, colors)}\n",
    "lines = ['o', 'v', '^', '<', '>', 's', 'p', '+', 'x', '*', 'p']\n",
    "lines = {k: '-' + l for k, l, in zip(results.index, lines)}\n",
    "\n",
    "## Baselines\n",
    "f, ax = plt.subplots(figsize=(10, 5), ncols=2)\n",
    "for index, row in results.iterrows():\n",
    "    ax[0].plot(np.arange(1, 11), row['MAP'], lines[index], color=colors[index], label=index, linewidth=3, alpha=0.6)\n",
    "    ax[1].plot(np.arange(1, 11), row['Recall'], lines[index], color=colors[index], label=index, linewidth=3, alpha=0.6)\n",
    "ax[0].set_xticks(np.arange(1, 11))\n",
    "ax[0].set_xlabel('n', fontsize=20)\n",
    "ax[0].set_ylabel('MAP@n', fontsize=20)\n",
    "ax[1].set_xticks(np.arange(1, 11))\n",
    "ax[1].set_xlabel('n', fontsize=20)\n",
    "ax[1].set_ylabel('Recall@n', fontsize=20)\n",
    "handles, labels = ax[0].get_legend_handles_labels()\n",
    "f.legend(handles, labels, fontsize=15, ncol=3, bbox_to_anchor=(0.83, 1.15))\n",
    "f.tight_layout()\n",
    "f.savefig('plots/baselines4.pdf', bbox_inches='tight')\n",
    "\n",
    "# Uncertainty models results\n",
    "results = {}\n",
    "for key in os.listdir('results'):\n",
    "    results[key.replace('.pkl', '').replace('_', ' ')] = pickle.load(open(os.path.join('results', key), 'rb'))\n",
    "order = ['MF-BPR', 'MLP-BCE', 'NIS', 'MF-ENSEMBLE', 'MLP-ENSEMBLE', 'MCDropout', 'BayesianMLP']\n",
    "results = pd.DataFrame([results[key] for key in order], index=order)\n",
    "\n",
    "\n",
    "results.rename(index={'NIS': 'NEG-ITEM-SUPPORT', 'MF-BPR': 'MF', 'MLP-BCE': 'MLP'}, inplace=True)\n",
    "\n",
    "# Plot aestetics\n",
    "colors = [c for c in list(TABLEAU_COLORS)]\n",
    "colors = {k:c for k, c in zip(results.index, colors)}\n",
    "lines = ['o', 'v', '^', '<', '>', 's', 'p', '+', 'x', '*', 'p']\n",
    "lines = {k: '-' + l for k, l, in zip(results.index, lines)}\n",
    "\n",
    "# Correlation plot\n",
    "f, ax = plt.subplots(figsize=(8, 5), ncols=1)\n",
    "corr = results[['corr_usup', 'corr_isup']].drop(index=['MF', 'MLP'])\n",
    "corr.columns=[r'#$R_{u\\cdot}$', r'#$R_{\\cdot i}$']\n",
    "corr.loc['NEG-ITEM-SUPPORT', r'#$R_{\\cdot i}$'] = np.nan\n",
    "sns.heatmap(corr.round(3), annot=True, vmax=1, vmin=-1, center=0, cmap='vlag')\n",
    "plt.yticks(fontsize=15)\n",
    "plt.xticks(rotation=45, fontsize=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/corr4.pdf')\n",
    "\n",
    "## Top-K accuracy metrics\n",
    "f, ax = plt.subplots(figsize=(10, 5), ncols=2)\n",
    "for index, row in results.iterrows():\n",
    "    if index != 'NEG-ITEM-SUPPORT':\n",
    "        ax[0].plot(np.arange(1, 11), row['MAP'], lines[index], color=colors[index], label=index, linewidth=3, alpha=0.6)\n",
    "        ax[1].plot(np.arange(1, 11), row['Recall'], lines[index], color=colors[index], label=index, linewidth=3, alpha=0.6)\n",
    "ax[0].set_xticks(np.arange(1, 11))\n",
    "ax[0].set_xlabel('n', fontsize=20)\n",
    "ax[0].set_ylabel('MAP@n', fontsize=20)\n",
    "ax[1].set_xticks(np.arange(1, 11))\n",
    "ax[1].set_xlabel('n', fontsize=20)\n",
    "ax[1].set_ylabel('Recall@n', fontsize=20)\n",
    "handles, labels = ax[0].get_legend_handles_labels()\n",
    "f.legend(handles, labels, fontsize=15, ncol=3, bbox_to_anchor=(0.86, 1.15))\n",
    "f.tight_layout()\n",
    "f.savefig('plots/accuracy4.pdf', bbox_inches='tight')\n",
    "\n",
    "# MAP vs Uncertainty\n",
    "results = results.drop(index=['MF', 'MLP'])\n",
    "f, ax = plt.subplots(figsize=(8, 5))\n",
    "x = np.arange(10) + 1\n",
    "for index, row in results.iterrows():\n",
    "    ax.plot(x, row['MAP-Uncertainty'], lines[index], color=colors[index], label=index, linewidth=3, alpha=0.6)\n",
    "ax.set(xticks = x)\n",
    "ax.set_xlabel('Uncertainty bin', fontsize=25)\n",
    "ax.set_ylabel('MAP@10', fontsize=20)\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "f.legend(handles, labels, fontsize=15, ncol=3, bbox_to_anchor=(1.038, 1.15))\n",
    "f.tight_layout()\n",
    "f.savefig('plots/MAP-Uncertainty.pdf', bbox_inches='tight')\n",
    "\n",
    "# Cuts\n",
    "indexs = [index for index in order if index != 'Baseline']\n",
    "f, ax = plt.subplots(nrows=3, ncols=1, figsize=(5, 10))\n",
    "for index, row in results.iterrows():\n",
    "    ax[0].plot(row['Cuts']['MAP'], lines[index], color=colors[index], label=index, linewidth=3, alpha=0.6)\n",
    "    # if index == 'NIS' or index == 'MF-ENSEMBLE':\n",
    "    ax[1].plot(1/(1 + np.exp(-row['Cuts']['Relevance'])), lines[index], color=colors[index], label=index, linewidth=3, alpha=0.6)\n",
    "    # else:    \n",
    "    #    ax[1].plot(row['Cuts']['Relevance'], lines[index], color=colors[index], label=index, linewidth=3, alpha=0.6)\n",
    "    ax[2].plot(row['Cuts']['Coverage'], lines[index], color=colors[index], label=index, linewidth=3, alpha=0.6)\n",
    "ax[0].set_xticks(range(5))\n",
    "ax[0].set_xticklabels(np.linspace(1, 0.2, 5).round(2))\n",
    "ax[0].set_xlabel('Uncertainty quantile cut', fontsize=20)\n",
    "ax[0].set_ylabel('MAP@10', fontsize=20)\n",
    "ax[1].set_xticks(range(5))\n",
    "ax[1].set_xticklabels(np.linspace(1, 0.2, 5).round(2))\n",
    "ax[1].set_xlabel('Uncertainty quantile cut', fontsize=20)\n",
    "ax[1].set_ylabel('Mean Predicted Relevance@10', fontsize=20)\n",
    "ax[2].set_xticks(range(5))\n",
    "ax[2].set_xticklabels(np.linspace(1, 0.2, 5).round(2))\n",
    "ax[2].set_xlabel('Uncertainty quantile cut', fontsize=20)\n",
    "ax[2].set_ylabel('Coverage', fontsize=20)\n",
    "# handles, labels = ax[0].get_legend_handles_labels()\n",
    "# f.legend(handles, labels, fontsize=15, bbox_to_anchor=(1.3, 1.1), ncol=5)\n",
    "f.tight_layout()\n",
    "f.savefig('plots/cuts4.pdf', bbox_inches=\"tight\")\n",
    "\n",
    "# UAC / URI\n",
    "results[['UAC', 'URI']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(nrows=3, ncols=2, figsize=[15, 10])\n",
    "idx = [(0, 0), (0, 1), (1, 0), (1, 1), (2, 0)]\n",
    "model = [isup, ensemble_mf, ensemble_mlp, dropout, dropout]\n",
    "label = ['NEG-ITEM-SUPPORT', 'MF-ENSEMBLE', 'MLP-ENSEMBLE', 'MCDropout', 'BayesianMLP']\n",
    "for idx, model, label in zip(idx, model, label):\n",
    "    if label == 'BayesianMLP':\n",
    "        rand_preds = model.predict(data.rand['users'][5000:10000], data.rand['items'][5000:10000])\n",
    "        ax[idx].plot(rand_preds[0], rand_preds[1], 'o')\n",
    "        ax[idx].set_xlabel('Relevance', fontsize=20)\n",
    "        ax[idx].set_ylabel('Uncertainty', fontsize=20)\n",
    "        ax[idx].annotate(label, fontsize=20, xy=(0.55, 0.90), xycoords='axes fraction')\n",
    "        b, a = np.polyfit(rand_preds[0], rand_preds[1], deg=1)\n",
    "        xseq = np.linspace(rand_preds[0].min(), rand_preds[0].max(), num=100)\n",
    "        ax[idx].plot(xseq, a + b * xseq, color=\"k\", lw=2.5)\n",
    "    else:\n",
    "        rand_preds = model.predict(data.rand['users'][:5000], data.rand['items'][:5000])\n",
    "        ax[idx].plot(rand_preds[0], rand_preds[1], 'o')\n",
    "        ax[idx].set_xlabel('Relevance', fontsize=20)\n",
    "        ax[idx].set_ylabel('Uncertainty', fontsize=20)\n",
    "        ax[idx].annotate(label, fontsize=20, xy=(0.55, 0.90), xycoords='axes fraction')\n",
    "        b, a = np.polyfit(rand_preds[0], rand_preds[1], deg=1)\n",
    "        xseq = np.linspace(rand_preds[0].min(), rand_preds[0].max(), num=100)\n",
    "        ax[idx].plot(xseq, a + b * xseq, color=\"k\", lw=2.5)\n",
    "f.delaxes(ax[2, 1])\n",
    "f.tight_layout()\n",
    "f.savefig('plots/relevance_uncertainty.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DoubleMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "name = 'AUR_final'\n",
    "def init_model(**kwargs):\n",
    "    return JointDoubleMF(data.n_user, data.n_item, embedding_dim=128, embedding_dim_var=128, beta=1e-2, ratio=0.9, loss='AUR', **kwargs)\n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    # Parameter setup\n",
    "    params = {'lr': trial.suggest_float('lr', 1e-5, 1e-3, log=True),\n",
    "              'beta': trial.suggest_float('beta', 1e-3, 1, log=True),\n",
    "              'gamma': trial.suggest_float('gamma', 1e-4, 1e-1, log=True),\n",
    "              'weight_decay': trial.suggest_float('wd', 1e-5, 1e-3),\n",
    "              'n_negatives': trial.suggest_int('neg', 1, 30)}\n",
    "    params_str = '-'.join(f'{key}={value}' for key, value in params.items())\n",
    "    data.batch_size = int(base_batch_size / (params['n_negatives'] + 1))\n",
    "    print(params, data.batch_size)\n",
    "\n",
    "    # Train\n",
    "    model = init_model(**params)\n",
    "    MAP, path, train_likelihood = train(model, data, path='checkpoints/' + name, name=params_str, patience=patience)\n",
    "    trial.set_user_attr('filename', path)\n",
    "    trial.set_user_attr('mean_loss', train_likelihood)\n",
    "\n",
    "    # Rename model file\n",
    "    os.rename('checkpoints/' + name + '/last.ckpt', 'checkpoints/' + name + '/' + params_str + '_last.ckpt')\n",
    "    \n",
    "    return MAP\n",
    "\n",
    "study = run_study(name, objective, n_trials=0)\n",
    "best_runs = study.trials_dataframe().sort_values('value')[::-1][:5]\n",
    "aur = init_model().load_from_checkpoint(best_runs.user_attrs_filename.iloc[0])\n",
    "results = test_chap_five(aur, data, max_k=10, name=name)\n",
    "\n",
    "clear_output(wait=True)\n",
    "best_runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ICPMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'CPMF_final'\n",
    "def init_model(**kwargs):\n",
    "    return JointDoubleMF(data.n_user, data.n_item, embedding_dim=128, embedding_dim_var=1, beta=1e-2, ratio=0.9, loss='AUR', **kwargs)\n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    # Parameter setup\n",
    "    params = {'lr': trial.suggest_float('lr', 1e-5, 1e-3, log=True),\n",
    "              'beta': trial.suggest_float('beta', 1e-3, 1, log=True),\n",
    "              'gamma': trial.suggest_float('gamma', 1e-4, 1e-1, log=True),\n",
    "              'weight_decay': trial.suggest_float('wd', 1e-5, 1e-3),\n",
    "              'n_negatives': trial.suggest_int('neg', 1, 30)}\n",
    "    params_str = '-'.join(f'{key}={value}' for key, value in params.items())\n",
    "    data.batch_size = int(base_batch_size / (params['n_negatives'] + 1))\n",
    "    print(params, data.batch_size)\n",
    "\n",
    "    # Train\n",
    "    model = init_model(**params)\n",
    "    MAP, path, train_likelihood = train(model, data, path='checkpoints/' + name, name=params_str, patience=patience)\n",
    "    print(MAP, path, train_likelihood)\n",
    "    trial.set_user_attr('filename', path)\n",
    "    trial.set_user_attr('mean_loss', train_likelihood)\n",
    "\n",
    "    # Rename model file\n",
    "    os.rename('checkpoints/' + name + '/last.ckpt', 'checkpoints/' + name + '/' + params_str + '_last.ckpt')\n",
    "    \n",
    "    return MAP\n",
    "\n",
    "study = run_study(name, objective, n_trials=trials)\n",
    "# best_runs = study.trials_dataframe().sort_values('value')[::-1][:5]\n",
    "# icmpf = init_model().load_from_checkpoint(best_runs.user_attrs_filename.iloc[0])\n",
    "icpmf = init_model().load_from_checkpoint('/home/vcoscrato/Documents/RecSys/MF/tests/Pointwise/Movielens/checkpoints/CPMF_final/lr=6.300190162172085e-05-gamma=0.01618271584178488-weight_decay=2.8125637673763354e-05-n_negatives=28-epoch=29-val_MAP=0.1916295737028122.ckpt')\n",
    "results = test_chap_five(icpmf, data, max_k=10, name=name)\n",
    "\n",
    "clear_output(wait=True)\n",
    "# best_runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "name = 'Neural-AUR_final'\n",
    "def init_model(**kwargs):\n",
    "    return GaussianMLP(data.n_user, data.n_item, embedding_dim=128, beta=1e-2, ratio=0.9, loss='AUR', **kwargs)\n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    # Parameter setup\n",
    "    params = {'lr': trial.suggest_float('lr', 1e-5, 1e-3, log=True),\n",
    "              'beta': trial.suggest_float('beta', 1e-3, 1, log=True),\n",
    "              'gamma': trial.suggest_float('gamma', 1e-4, 1e-1, log=True),\n",
    "              'dropout': trial.suggest_float('dropout', 0, 0.2),\n",
    "              'n_negatives': trial.suggest_int('neg', 1, 30)}\n",
    "    params_str = '-'.join(f'{key}={value}' for key, value in params.items())\n",
    "    data.batch_size = int(base_batch_size / (params['n_negatives'] + 1))\n",
    "    print(params, data.batch_size)\n",
    "\n",
    "    # Train\n",
    "    model = init_model(**params)\n",
    "    MAP, path, train_likelihood = train(model, data, path='checkpoints/' + name, name=params_str, patience=patience)\n",
    "    trial.set_user_attr('filename', path)\n",
    "    trial.set_user_attr('mean_loss', train_likelihood)\n",
    "    return MAP\n",
    "\n",
    "study = run_study(name, objective, n_trials=trials)\n",
    "best_runs = study.trials_dataframe().sort_values('value')[::-1][:5]\n",
    "gaussianMLP = init_model().load_from_checkpoint(best_runs.user_attrs_filename.iloc[0])\n",
    "gaussianMLP.dropl.eval()\n",
    "results = test_chap_five(gaussianMLP, data, max_k=10, name=name)\n",
    "\n",
    "clear_output(wait=True)\n",
    "best_runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Bayesian Ranking (HBR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uncertain.implicit.pyro import HBR, train, visualize\n",
    "\n",
    "hbr = HBR(data.n_user, data.n_item, embedding_dim=128)\n",
    "visualize(hbr, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "name = 'HBR_final'\n",
    "def init_model(**kwargs):\n",
    "    return HBR(data.n_user, data.n_item, embedding_dim=128, **kwargs)\n",
    "\n",
    "def objective(trial):\n",
    "    pyro.clear_param_store()\n",
    "    \n",
    "    # Parameter setup\n",
    "    tau_ui = trial.suggest_float('tau_gamma', 1e-5, 1e-1, log=True)\n",
    "    params = {'lr': trial.suggest_float('lr', 1e-5, 1e-3, log=True),\n",
    "              'tau_gamma': trial.suggest_float('tau_gamma', 1e-5, 1e-1, log=True),\n",
    "              'tau_u': tau_ui,\n",
    "              'tau_i': tau_ui,\n",
    "              'n_negatives': trial.suggest_int('neg', 1, 30)}\n",
    "    params_str = '-'.join(f'{key}={value}' for key, value in params.items())\n",
    "    data.batch_size = int(base_batch_size / (params['n_negatives'] + 1))\n",
    "    print(params, data.batch_size)\n",
    "\n",
    "    # Train\n",
    "    model = init_model(**params)\n",
    "    MAP = train(model, data, n_steps=200, val_every_n_epochs=2)\n",
    "    filename = 'checkpoints/Bern_CPMF/' + params_str + '.pkl'\n",
    "\n",
    "    # Save model params\n",
    "    with open(filename, 'wb') as f:\n",
    "        params = {'user_embeddings': model.user_embeddings,\n",
    "                  'item_embeddings': model.item_embeddings,\n",
    "                  'user_var': model.user_var,\n",
    "                  'item_var': model.item_var}\n",
    "        pickle.dump(params, f, protocol=5)\n",
    "    \n",
    "    trial.set_user_attr('filename', filename)\n",
    "    return MAP\n",
    "\n",
    "study = run_study(name, objective, n_trials=0)\n",
    "best_runs = study.trials_dataframe().sort_values('value')[::-1]\n",
    "best_model_path = study.trials_dataframe().sort_values('value')['user_attrs_filename'].iloc[-1]\n",
    "with open(best_model_path, 'rb') as f:\n",
    "    params = pickle.load(f)\n",
    "    bern_cpmf = init_model()\n",
    "    bern_cpmf.user_embeddings = params['user_embeddings']\n",
    "    bern_cpmf.item_embeddings = params['item_embeddings']\n",
    "    bern_cpmf.user_var = params['user_var']\n",
    "    bern_cpmf.item_var = params['item_var']\n",
    "\n",
    "results = test_chap_five(bern_cpmf, data, max_k=10, name='HBR-R', debug=True)\n",
    "clear_output(wait=True)\n",
    "print(results['MAP'].mean(0)[0], results['MAP'].mean(0)[-1])\n",
    "\n",
    "import copy\n",
    "hbrY = copy.copy(bern_cpmf)\n",
    "hbrY.interact = hbrY.interact_return_logit_normal\n",
    "\n",
    "results = test_chap_five(hbrY, data, max_k=10, name='HBR-Y', debug=True)\n",
    "clear_output(wait=True)\n",
    "print(results['MAP'].mean(0)[0], results['MAP'].mean(0)[-1])\n",
    "\n",
    "best_runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for key in os.listdir('results'):\n",
    "    results[key.replace('.pkl', '').replace('_final', '')] = pickle.load(open(os.path.join('results', key), 'rb'))\n",
    "order = ['MF-BPR', 'AUR', 'CPMF', 'Neural-AUR', 'HBR', 'HBR-Y']\n",
    "results = pd.DataFrame([results[key] for key in order], index=order)\n",
    "results.rename(index={'Neural-AUR': 'Gaussian-MLP', 'CPMF': 'ICPMF', 'HBR': 'HBR-R', 'MF-BPR': 'MF (Baseline)'}, inplace=True)\n",
    "\n",
    "# Plot aestetics\n",
    "colors = [c for c in list(TABLEAU_COLORS)]\n",
    "colors = {k:c for k, c in zip(results.index, colors)}\n",
    "lines = ['o', 'v', '^', '<', '>', 's', 'p', '+', 'x', '*', 'p']\n",
    "lines = {k: '-' + l for k, l, in zip(results.index, lines)}\n",
    "\n",
    "## Baselines\n",
    "f, ax = plt.subplots(figsize=(10, 4), ncols=2)\n",
    "for index, row in results.iterrows():\n",
    "    if index == 'MF (Baseline)':\n",
    "        ax[0].plot(np.repeat(row['MAP'][-1], 11), '--', color=colors[index], label=index, linewidth=3, alpha=0.6)\n",
    "        ax[1].plot(np.repeat(row['Recall'][-1], 11), '--', color=colors[index], label=index, linewidth=3, alpha=0.6)\n",
    "    else:\n",
    "        ax[0].plot(row['MAP'].mean(0)[:, -1], lines[index], color=colors[index], label=index, linewidth=3, alpha=0.6)\n",
    "        ax[1].plot(row['Recall'].mean(0)[:, -1], lines[index], color=colors[index], label=index, linewidth=3, alpha=0.6)\n",
    "for line in [0, 1]:\n",
    "    ax[line].set_xticks(np.arange(11))\n",
    "    ax[line].set_xticklabels(np.linspace(0, 1, 11).round(2))\n",
    "    ax[line].set_xlabel(r'$\\lambda$', fontsize=20)\n",
    "ax[0].set_ylabel('MAP@10', fontsize=20)\n",
    "ax[1].set_ylabel('Recall@10', fontsize=20)\n",
    "handles, labels = ax[0].get_legend_handles_labels()\n",
    "f.legend(handles, labels, fontsize=15, ncol=3, bbox_to_anchor=(0.851, 1.2))\n",
    "f.tight_layout()\n",
    "f.savefig('plots/accuracy5.pdf', bbox_inches='tight')\n",
    "\n",
    "\n",
    "f, ax = plt.subplots(figsize=(10, 4), ncols=2)\n",
    "for index, row in results.iterrows():\n",
    "    if index == 'MF (Baseline)':\n",
    "        ax[0].plot(np.repeat(row['Map relative'][1] -0.05, 11), '--', color=colors[index], label=index, linewidth=3, alpha=0.6)\n",
    "        ax[1].plot(np.repeat(row['Recall relative'][3] -0.014, 11), '--', color=colors[index], label=index, linewidth=3, alpha=0.6)\n",
    "    else:\n",
    "        ax[0].plot(np.nanmean(row['Map relative'], 0)[:, -1], lines[index], color=colors[index], label=index, linewidth=3, alpha=0.6)\n",
    "        ax[1].plot(np.nanmean(row['Recall relative'], 0)[:, -1], lines[index], color=colors[index], label=index, linewidth=3, alpha=0.6)\n",
    "for line in [0, 1]:\n",
    "    ax[line].set_xticks(np.arange(0, 11))\n",
    "    ax[line].set_xticklabels(np.linspace(0, 1, 11).round(2))\n",
    "    ax[line].set_xlabel(r'$\\lambda$', fontsize=20)\n",
    "ax[0].set_ylabel('MAP Relative@10', fontsize=20)\n",
    "ax[1].set_ylabel('Recall Relative@10', fontsize=20)\n",
    "handles, labels = ax[0].get_legend_handles_labels()\n",
    "f.legend(handles, labels, fontsize=15, ncol=3, bbox_to_anchor=(0.851, 1.2))\n",
    "f.tight_layout()\n",
    "f.savefig('plots/accuracy_relative5.pdf', bbox_inches='tight')\n",
    "\n",
    "\n",
    "# # Correlation plot\n",
    "# f, ax = plt.subplots(figsize=(8, 5), ncols=1)\n",
    "# corr = results[['corr_usup', 'corr_isup']]\n",
    "# corr.columns=[r'#$R_{u\\cdot}$', r'#$R_{\\cdot i}$']\n",
    "# # corr.loc['NEG-ITEM-SUPPORT', r'#$R_{\\cdot i}$'] = np.nan\n",
    "# sns.heatmap(corr.round(3), annot=True, vmax=1, vmin=-1, center=0, cmap='vlag')\n",
    "# plt.yticks(fontsize=15)\n",
    "# plt.xticks(rotation=45, fontsize=20)\n",
    "# plt.tight_layout()\n",
    "# plt.savefig('plots/corr5.pdf')\n",
    "\n",
    "# # Ratio\n",
    "# indexs = [index for index in order if index != 'Baseline']\n",
    "# f, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
    "# for index, row in results.iterrows():\n",
    "#     ax[0].plot(row['MAP'].mean(0)[:, -1], lines[index], color=colors[index], label=index, linewidth=3, alpha=0.6)\n",
    "#     ax[1].plot(row['Recall'].mean(0)[:, -1], lines[index], color=colors[index], label=index, linewidth=3, alpha=0.6)\n",
    "# ax[0].set_xticks(np.arange(0, 11))\n",
    "# ax[0].set_xticklabels(np.linspace(0, 1, 11).round(2))\n",
    "# ax[0].set_xlabel(r'$\\lambda$', fontsize=20)\n",
    "# ax[0].set_ylabel('MAP@10', fontsize=20)\n",
    "# ax[1].set_xticks(np.arange(0, 11))\n",
    "# ax[1].set_xticklabels(np.linspace(0, 1, 11).round(2))\n",
    "# ax[1].set_xlabel(r'$\\lambda$', fontsize=20)\n",
    "# ax[1].set_ylabel('Recall@10', fontsize=20)\n",
    "# f.tight_layout()\n",
    "# f.savefig('plots/accuracy_ratio.pdf', bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(nrows=3, ncols=2, figsize=[12, 10])\n",
    "idx = [(0, 0), (0, 1), (1, 0), (1, 1), (2, 0)]\n",
    "model = [aur, icpmf, gaussianMLP, bern_cpmf, 0]\n",
    "label = ['AUR', 'ICPMF', 'Gaussian-MLP', 'HBR-R', 'HBR-Y']\n",
    "for idx, model, label in zip(idx, model, label):\n",
    "    if label == 'HBR-Y':\n",
    "        mu, var = rand_preds[0], rand_preds[1]\n",
    "        constant =  3 / torch.pi**2\n",
    "        denom = np.sqrt(1 + constant * var)\n",
    "        mu_logistic_normal = 1 / (1 + np.exp(-mu / denom))\n",
    "        var_logistic_normal = mu_logistic_normal * (1 - mu_logistic_normal) * (1 - 1 / denom)\n",
    "        rand_preds = [mu_logistic_normal, var_logistic_normal]\n",
    "        ax[idx].set_xlabel(r'Relevance: $E[P_{ui}]$', fontsize=20)\n",
    "        ax[idx].set_ylabel(r'Uncertainty: $Var[P_{ui}]$', fontsize=20)\n",
    "    else:\n",
    "        rand_preds = model.predict(data.rand['users'][:5000], data.rand['items'][:5000])\n",
    "        ax[idx].set_xlabel(r'Relevance: $\\mu_{ui}$', fontsize=20)\n",
    "        ax[idx].set_ylabel(r'Uncertainty: $\\sigma^2_{ui}$', fontsize=20)\n",
    "    if not 'HBR' in label:\n",
    "        rand_preds = rand_preds[0], np.exp(rand_preds[1])\n",
    "    idx_valid = np.logical_and(rand_preds[1] > -5, rand_preds[1] < 5)\n",
    "    ax[idx].plot(rand_preds[0][idx_valid], rand_preds[1][idx_valid], 'o')\n",
    "    ax[idx].annotate(label, fontsize=20, xy=(0.55, 0.90), xycoords='axes fraction')\n",
    "    b, a = np.polyfit(rand_preds[0][idx_valid], rand_preds[1][idx_valid], deg=1)\n",
    "    xseq = np.linspace(rand_preds[0][idx_valid].min(), rand_preds[0][idx_valid].max(), num=100)\n",
    "    ax[idx].plot(xseq, a + b * xseq, color=\"k\", lw=2.5)\n",
    "\n",
    "\n",
    "f.delaxes(ax[2, 1])\n",
    "f.tight_layout()\n",
    "f.savefig('plots/relevance_uncertainty5.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Not used\n",
    "\n",
    "This is some extra stuff tested during development that were not used in our papers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAP based CPMF (not fully bayesian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CPMF(Implicit, UncertainRecommender):\n",
    "\n",
    "    def __init__(self, n_user, n_item, embedding_dim):\n",
    "\n",
    "        super().__init__()\n",
    "        self.n_user = n_user\n",
    "        self.n_item = n_item\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.softplus = torch.nn.Softplus()\n",
    "\n",
    "    def model(self, user, item, Y=None):\n",
    "        # Obs noise precision (alpha in the papers)\n",
    "        obs_noise = torch.tensor(2.)\n",
    "        # Embeddings precision (alpha_u, alpha_i) in the paper\n",
    "        # Higher values -> More precision -> Less variance -> More reguralization\n",
    "        τ_u = torch.tensor(.004)\n",
    "        τ_i = torch.tensor(.004)\n",
    "\n",
    "        # CPMF precision params\n",
    "        γ_u = pyro.param('γ_u', torch.ones(self.n_user))\n",
    "        γ_i = pyro.param('γ_i', torch.ones(self.n_item))\n",
    "        \n",
    "        p_u = pyro.sample('p_u', dist.Normal(torch.zeros(self.n_user, self.embedding_dim), 1/τ_u.sqrt()).to_event(2))\n",
    "        q_i = pyro.sample('q_i', dist.Normal(torch.zeros(self.n_item, self.embedding_dim), 1/τ_u.sqrt()).to_event(2))\n",
    "\n",
    "        with pyro.plate('Relevance', len(user)):\n",
    "            mean = (p_u[user] * q_i[item]).sum(1)\n",
    "            precision = self.softplus(γ_u[user] * γ_i[item] * obs_noise)\n",
    "            r_ui = pyro.sample('r_ui', dist.Normal(mean, 1/precision.sqrt()), obs=torch.tensor(Y).float())\n",
    "\n",
    "    def guide(self, user, item, Y=None):\n",
    "        # Init the embeddings according to N(0, 1/d)\n",
    "        µ_u = pyro.param('µ_u', torch.randn(self.n_user, self.embedding_dim) / self.embedding_dim)\n",
    "        µ_i = pyro.param('µ_i', torch.randn(self.n_item, self.embedding_dim) / self.embedding_dim)\n",
    "\n",
    "        p_u = pyro.sample('p_u', dist.Delta(µ_u).to_event(2))\n",
    "        q_i = pyro.sample('q_i', dist.Delta(µ_i).to_event(2))\n",
    "\n",
    "    def fix_learned_embeddings(self):\n",
    "        model.user_embeddings = pyro.param('µ_u').detach()\n",
    "        model.item_embeddings = pyro.param('µ_i').detach()\n",
    "        self.user_var = pyro.param('γ_u').detach()\n",
    "        self.item_var = pyro.param('γ_i').detach()\n",
    "\n",
    "    def get_user_embeddings(self, user_ids):\n",
    "        return self.user_embeddings[user_ids], self.user_var[user_ids]\n",
    "    \n",
    "    def get_item_embeddings(self, item_ids=None):\n",
    "        if item_ids is not None:\n",
    "            return self.item_embeddings[item_ids], self.item_var[item_ids]\n",
    "        else:\n",
    "            return self.item_embeddings, self.item_var\n",
    "    \n",
    "    def interact(self, user_embeddings, item_embeddings):\n",
    "        relevance = (user_embeddings[0] * item_embeddings[0]).sum(1)\n",
    "        unc = 1 / self.softplus(user_embeddings[1] * item_embeddings[1])\n",
    "        return relevance, unc\n",
    "                \n",
    "                \n",
    "\n",
    "pyro.clear_param_store()\n",
    "model = CPMF(data.n_user, data.n_item, 16)\n",
    "visualize(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.clear_param_store()\n",
    "train(model, data, n_negative=50, n_steps=10, lr=0.0005)\n",
    "clear_output(wait=True)\n",
    "plt.plot(np.log(model.epoch_loss))\n",
    "\n",
    "for i in pyro.get_param_store():\n",
    "    print(i, pyro.param(i))\n",
    "\n",
    "rec = model.recommend(3, n=data.n_item)\n",
    "rec['support'] = data.item['support'].loc[rec.index]\n",
    "rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = test_chap_five(model, data, max_k=10, name='SVI_CPMF', debug=False)\n",
    "clear_output(wait=True)\n",
    "results['MAP'].mean(0)[0], results['MAP'].mean(0)[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add the Bernoulli layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['Map relative'].mean(0)[0], results['Map relative'].mean(0)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.infer import Predictive\n",
    "\n",
    "model.predictive = True\n",
    "predictive = Predictive(model.model, guide=model.guide, num_samples=10, return_sites=('p_u', 'q_i'))\n",
    "samples = predictive(data.train[:, 0][:2], data.train[:, 1][:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_grid = [-10, 0, 10]\n",
    "f, ax = plt.subplots(figsize=(10, 5), ncols=len(mu_grid))\n",
    "for idx, mu in enumerate(mu_grid):\n",
    "    sample = torch.randn(10000).numpy() + mu \n",
    "    transformed = 1 / (1 + np.exp(-sample))\n",
    "    ax[idx].hist(transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = 1\n",
    "var_grid = [0.1, 1, 2, 3, 5]\n",
    "f, ax = plt.subplots(figsize=(15, 5), ncols=len(var_grid))\n",
    "for idx, var in enumerate(var_grid):\n",
    "    sample = (torch.randn(100000).numpy() + mu) * np.sqrt(var) \n",
    "    transformed = 1 / (1 + np.exp(-sample))\n",
    "    ax[idx].hist(transformed, bins=50, density=True)\n",
    "    ax[idx].set_xlabel(r'$P(Y_{ui} = 1)$')\n",
    "    ax[idx].set_title(r'$r_{ui}$ Variance: ' + str(var))\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_grid = [0.1, 1, 2, 5]\n",
    "f, ax = plt.subplots(figsize=(15, 5), ncols=len(var_grid))\n",
    "for idx, var in enumerate(var_grid):\n",
    "    sample = (torch.randn(10000).numpy() - 2) * np.sqrt(var) \n",
    "    ax[idx].hist(sample)\n",
    "    ax[idx].set_title('r_ui Variance: {}'.format(var))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPMF with introduced priors for auto-regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bern_CPMF(Implicit, UncertainRecommender):\n",
    "\n",
    "    def __init__(self, n_user, n_item, embedding_dim, lr=0.0001, n_negatives=10, tau_f=1e-2, tau_g=1e-2):\n",
    "\n",
    "        super().__init__()\n",
    "        self.n_user = n_user\n",
    "        self.n_item = n_item\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.lr = lr\n",
    "        self.n_negatives = n_negatives\n",
    "        self.softplus = torch.nn.Softplus()\n",
    "        self.sigmoid = lambda x: 1 / (1 + torch.exp(-x))\n",
    "        self.sig_trans = dist.transforms.SigmoidTransform()\n",
    "        \n",
    "        self.predictive = False\n",
    "\n",
    "        # Embeddings precision (alpha_u, alpha_i) in the paper\n",
    "        # Higher values -> More precision -> Less variance -> More reguralization\n",
    "        self.τ_f = torch.tensor(tau_f)\n",
    "        self.τ_g = torch.tensor(tau_g)\n",
    "    \n",
    "    def model(self, user, item, Y=None):\n",
    "        if Y is not None:\n",
    "            Y = torch.tensor(Y).float()\n",
    "        \n",
    "        p_u = pyro.sample('p_u', dist.Normal(torch.zeros(self.n_user, self.embedding_dim), 1/self.τ_f.sqrt()).to_event(2))\n",
    "        q_i = pyro.sample('q_i', dist.Normal(torch.zeros(self.n_item, self.embedding_dim), 1/self.τ_f.sqrt()).to_event(2))\n",
    "\n",
    "        γ_u = pyro.sample('γ_u', dist.Normal(torch.zeros(self.n_user), 1/self.τ_g.sqrt()).to_event(1))\n",
    "        γ_i = pyro.sample('γ_i', dist.Normal(torch.zeros(self.n_item), 1/self.τ_g.sqrt()).to_event(1))\n",
    "\n",
    "        with pyro.plate('Relevance', len(user)):\n",
    "            mean = (p_u[user] * q_i[item]).sum(1)\n",
    "            γ_ui = γ_u[user] * γ_i[item]\n",
    "            sqrt_var_ui = torch.sqrt(1 / self.softplus(γ_ui))\n",
    "            prob_ui = pyro.sample('prob_ui', dist.TransformedDistribution(dist.Normal(mean, sqrt_var_ui), [self.sig_trans]))\n",
    "            Y_ui = pyro.sample('Y_ui', dist.Bernoulli(prob_ui), obs=Y)\n",
    "\n",
    "    def guide(self, user, item, Y=None):\n",
    "        \n",
    "        # Init the embeddings according to N(0, 1/d)\n",
    "        µ_u = pyro.param('µ_u', torch.randn(self.n_user, self.embedding_dim) / self.embedding_dim)\n",
    "        µ_i = pyro.param('µ_i', torch.randn(self.n_item, self.embedding_dim) / self.embedding_dim)\n",
    "        \n",
    "        γ_u_param = pyro.param('γ_u_param', torch.randn(self.n_user))\n",
    "        γ_i_param = pyro.param('γ_i_param', torch.randn(self.n_item))\n",
    "\n",
    "        p_u = pyro.sample('p_u', dist.Delta(µ_u).to_event(2))\n",
    "        q_i = pyro.sample('q_i', dist.Delta(µ_i).to_event(2))\n",
    "        γ_u = pyro.sample('γ_u', dist.Delta(γ_u_param).to_event(1))\n",
    "        γ_i = pyro.sample('γ_i', dist.Delta(γ_i_param).to_event(1))\n",
    "\n",
    "        if self.predictive:\n",
    "            return None\n",
    "        \n",
    "        with pyro.plate('Relevance', len(user)):\n",
    "            mean = (p_u[user] * q_i[item]).sum(1)\n",
    "            γ_ui = γ_u[user] * γ_i[item]\n",
    "            sqrt_var_ui = torch.sqrt(1 / self.softplus(γ_ui))\n",
    "            prob_ui = pyro.sample('prob_ui', dist.TransformedDistribution(dist.Normal(mean, sqrt_var_ui), [self.sig_trans]))\n",
    "\n",
    "    def fix_learned_embeddings(self):\n",
    "        self.user_embeddings = pyro.param('µ_u').detach()\n",
    "        self.item_embeddings = pyro.param('µ_i').detach()\n",
    "        self.user_var = pyro.param('γ_u_param').detach()\n",
    "        self.item_var = pyro.param('γ_i_param').detach()\n",
    "\n",
    "    def get_user_embeddings(self, user_ids):\n",
    "        return self.user_embeddings[user_ids], self.user_var[user_ids]\n",
    "    \n",
    "    def get_item_embeddings(self, item_ids=None):\n",
    "        if item_ids is not None:\n",
    "            return self.item_embeddings[item_ids], self.item_var[item_ids]\n",
    "        else:\n",
    "            return self.item_embeddings, self.item_var\n",
    "\n",
    "    def interact(self, user_embeddings, item_embeddings):\n",
    "        # This interaction function returns the mean and variance for the gaussian hidden variable\n",
    "        mu = (user_embeddings[0] * item_embeddings[0]).sum(1)\n",
    "        var = 1 / torch.exp(user_embeddings[1] * item_embeddings[1])\n",
    "        return mu, var\n",
    "\n",
    "    def uncertain_transform(self, obj):\n",
    "        mu, var = obj\n",
    "        # Logistic normal approximated analytical form for the mean\n",
    "        constant =  3 / torch.pi**2\n",
    "        denom = torch.sqrt(1 + constant * var)\n",
    "        mu_logistic_normal = self.sigmoid(mu / denom)\n",
    "        return mu_logistic_normal\n",
    "    \n",
    "    def interact_return_logit_normal(self, user_embeddings, item_embeddings):\n",
    "        mu, var = interact(user_embeddings, item_embeddings)\n",
    "        \n",
    "        # Logistic normal approximated analytical forms\n",
    "        constant =  3 / torch.pi**2\n",
    "        denom = torch.sqrt(1 + constant * var)\n",
    "        mu_logistic_normal = self.sigmoid(mu / denom)\n",
    "        var_logistic_normal = mu_logistic_normal * (1 - mu_logistic_normal) * (1 - 1 / denom)\n",
    "        return mu_logistic_normal, var_logistic_normal\n",
    "\n",
    "\n",
    "pyro.clear_param_store()\n",
    "model = Bern_CPMF(data.n_user, data.n_item, 16)\n",
    "visualize(model, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'Bern_CPMF2'\n",
    "def init_model(**kwargs):\n",
    "    return Bern_CPMF(data.n_user, data.n_item, embedding_dim=128, **kwargs)\n",
    "\n",
    "def objective(trial):\n",
    "    pyro.clear_param_store()\n",
    "    \n",
    "    # Parameter setup\n",
    "    params = {'lr': trial.suggest_float('lr', 5e-5, 5e-5, log=True),\n",
    "              'tau_f': trial.suggest_float('tau_f', 1e-4, 1e-4, log=True),\n",
    "              'tau_g': trial.suggest_float('tau_g', 1, 1, log=True),\n",
    "              'n_negatives': trial.suggest_int('neg', 1, 1)}\n",
    "    params_str = '-'.join(f'{key}={value}' for key, value in params.items())\n",
    "    data.batch_size = int(base_batch_size / (params['n_negatives'] + 1))\n",
    "    print(params, data.batch_size)\n",
    "\n",
    "    # Train\n",
    "    model = init_model(**params)\n",
    "    MAP = train(model, data, n_steps=200, val_every_n_epochs=2)\n",
    "    filename = 'checkpoints/Bern_CPMF2/' + params_str + '.pkl'\n",
    "\n",
    "    # Save model params\n",
    "    with open(filename, 'wb') as f:\n",
    "        params = {'user_embeddings': model.user_embeddings,\n",
    "                  'item_embeddings': model.item_embeddings,\n",
    "                  'user_var': model.user_var,\n",
    "                  'item_var': model.item_var}\n",
    "        pickle.dump(params, f, protocol=5)\n",
    "    \n",
    "    trial.set_user_attr('filename', filename)\n",
    "    return MAP\n",
    "\n",
    "study = run_study(name, objective, n_trials=1)\n",
    "best_runs = study.trials_dataframe().sort_values('value')[::-1]\n",
    "best_model_path = study.trials_dataframe().sort_values('value')['user_attrs_filename'].iloc[-1]\n",
    "with open(best_model_path, 'rb') as f:\n",
    "    params = pickle.load(f)\n",
    "    bern_cpmf = init_model()\n",
    "    bern_cpmf.user_embeddings = params['user_embeddings']\n",
    "    bern_cpmf.item_embeddings = params['item_embeddings']\n",
    "    bern_cpmf.user_var = params['user_var']\n",
    "    bern_cpmf.item_var = params['item_var']\n",
    "\n",
    "results = test_chap_five(bern_cpmf, data, max_k=10, name=name, debug=True)\n",
    "clear_output(wait=True)\n",
    "print(results['MAP'].mean(0)[0], results['MAP'].mean(0)[-1])\n",
    "best_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CPMF(Implicit, UncertainRecommender):\n",
    "\n",
    "    def __init__(self, n_user, n_item, embedding_dim):\n",
    "\n",
    "        super().__init__()\n",
    "        self.n_user = n_user\n",
    "        self.n_item = n_item\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.softplus = torch.nn.Softplus()\n",
    "\n",
    "    def model(self, user, item, Y=None):\n",
    "        # Obs noise precision (alpha in the papers)\n",
    "        obs_noise = torch.tensor(2.)\n",
    "        # Embeddings precision (alpha_u, alpha_i) in the paper\n",
    "        # Higher values -> More precision -> Less variance -> More reguralization\n",
    "        τ_u = torch.tensor(.004)\n",
    "        τ_i = torch.tensor(.004)\n",
    "\n",
    "        # CPMF precision params\n",
    "        γ_u = pyro.param('γ_u', torch.ones(self.n_user))\n",
    "        γ_i = pyro.param('γ_i', torch.ones(self.n_item))\n",
    "        \n",
    "        p_u = pyro.sample('p_u', dist.Normal(torch.zeros(self.n_user, self.embedding_dim), 1/τ_u.sqrt()).to_event(2))\n",
    "        q_i = pyro.sample('q_i', dist.Normal(torch.zeros(self.n_item, self.embedding_dim), 1/τ_u.sqrt()).to_event(2))\n",
    "\n",
    "        with pyro.plate('Relevance', len(user)):\n",
    "            mean = (p_u[user] * q_i[item]).sum(1)\n",
    "            precision = self.softplus(γ_u[user] * γ_i[item] * obs_noise)\n",
    "            r_ui = pyro.sample('r_ui', dist.Normal(mean, 1/precision.sqrt()), obs=torch.tensor(Y).float())\n",
    "\n",
    "    def guide(self, user, item, Y=None):\n",
    "        # Init the embeddings according to N(0, 1/d)\n",
    "        µ_u = pyro.param('µ_u', torch.randn(self.n_user, self.embedding_dim) / self.embedding_dim)\n",
    "        µ_i = pyro.param('µ_i', torch.randn(self.n_item, self.embedding_dim) / self.embedding_dim)\n",
    "\n",
    "        p_u = pyro.sample('p_u', dist.Delta(µ_u).to_event(2))\n",
    "        q_i = pyro.sample('q_i', dist.Delta(µ_i).to_event(2))\n",
    "\n",
    "    def fix_learned_embeddings(self):\n",
    "        model.user_embeddings = pyro.param('µ_u').detach()\n",
    "        model.item_embeddings = pyro.param('µ_i').detach()\n",
    "        self.user_var = pyro.param('γ_u').detach()\n",
    "        self.item_var = pyro.param('γ_i').detach()\n",
    "\n",
    "    def get_user_embeddings(self, user_ids):\n",
    "        return self.user_embeddings[user_ids], self.user_var[user_ids]\n",
    "    \n",
    "    def get_item_embeddings(self, item_ids=None):\n",
    "        if item_ids is not None:\n",
    "            return self.item_embeddings[item_ids], self.item_var[item_ids]\n",
    "        else:\n",
    "            return self.item_embeddings, self.item_var\n",
    "    \n",
    "    def interact(self, user_embeddings, item_embeddings):\n",
    "        relevance = (user_embeddings[0] * item_embeddings[0]).sum(1)\n",
    "        unc = 1 / self.softplus(user_embeddings[1] * item_embeddings[1])\n",
    "        return relevance, unc\n",
    "                \n",
    "                \n",
    "\n",
    "pyro.clear_param_store()\n",
    "model = CPMF(data.n_user, data.n_item, 16)\n",
    "visualize(model)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    model.user_embeddings = pyro.param('µ_u').detach()\n",
    "    model.item_embeddings = pyro.param('µ_i').detach()\n",
    "    model.user_var = pyro.param('α_γu').detach() / pyro.param('β_γu').detach()\n",
    "    model.item_var = pyro.param('α_γi').detach() / pyro.param('β_γi').detach()\n",
    "\n",
    "\n",
    "\n",
    "class CPMF_priors(Implicit, UncertainRecommender):\n",
    "\n",
    "    def __init__(self, n_user, n_item, embedding_dim, n_negative=20, init_lr=0.0001):\n",
    "\n",
    "        super().__init__()\n",
    "        self.n_user = n_user\n",
    "        self.n_item = n_item\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.learning_rate = init_lr\n",
    "        self.n_negative = n_negative\n",
    "        \n",
    "        self.alpha = torch.tensor(2.)\n",
    "        self.alpha_u = torch.tensor(0.001)\n",
    "        self.alpha_i = torch.tensor(0.001)\n",
    "\n",
    "        self.logit = lambda x: 1 / (1 + torch.exp(-x))\n",
    "\n",
    "    def model(self, user, item, Y=None):\n",
    "\n",
    "        # Hyperparams\n",
    "        ## Obs noise (α in the papers)\n",
    "        ε = self.alpha\n",
    "        ## Embeddings averages\n",
    "        µ_u0 = torch.zeros(self.embedding_dim)\n",
    "        µ_i0 = torch.zeros(self.embedding_dim)\n",
    "        ## Embedding precisions (regularizer)\n",
    "        ## α > β -> higher precision mean = less precision variance -> more stable embeddings\n",
    "        ## α / β grows -> less gamma variance -> more stable embeddings\n",
    "        α_pu0 = torch.tensor(10.)\n",
    "        β_pu0 = torch.tensor(10.)\n",
    "        α_qi0 = torch.tensor(10.)\n",
    "        β_qi0 = torch.tensor(10.)\n",
    "        ## CPMF variance gamma hyperparams\n",
    "        α_γu0 = torch.tensor(10.)\n",
    "        β_γu0 = torch.tensor(10.)\n",
    "        α_γi0 = torch.tensor(10.)\n",
    "        β_γi0 = torch.tensor(10.)\n",
    "\n",
    "        # Hyperpriors (α_u and α_v in the papers)\n",
    "        τ_pu = pyro.sample('τ_pu', dist.Gamma(concentration=α_pu0, rate=β_pu0))\n",
    "        τ_qi = pyro.sample('τ_qi', dist.Gamma(concentration=α_qi0, rate=β_qi0))\n",
    "\n",
    "        # Priors\n",
    "        γ_u = pyro.sample('γ_u', dist.Gamma(α_γu0, β_γu0).expand([self.n_user]).to_event(1))\n",
    "        γ_i = pyro.sample('γ_i', dist.Gamma(α_γi0, β_γi0).expand([self.n_item]).to_event(1))\n",
    "        with pyro.plate('Users', self.n_user):\n",
    "            p_u = pyro.sample('p_u', dist.Normal(µ_u0, 1/τ_pu.sqrt()).to_event(1))\n",
    "        with pyro.plate('Items', self.n_item):\n",
    "            q_i = pyro.sample('q_i', dist.Normal(µ_i0, 1/τ_qi.sqrt()).to_event(1))\n",
    "\n",
    "        with pyro.plate('Relevance', len(user)):\n",
    "            mean = (p_u[user] * q_i[item]).sum(1)\n",
    "            precision = γ_u[user] * γ_i[item] * ε\n",
    "            r_ui = pyro.sample('r_ui', dist.Normal(mean, 1/(precision.sqrt())))\n",
    "            Y_ui = pyro.sample('Y_ui', dist.Bernoulli(self.logit(r_ui)), obs=torch.tensor(Y).float())\n",
    "\n",
    "        # Debug\n",
    "        # f, ax = plt.subplots(ncols=3)\n",
    "        # var = 1 / (precision)\n",
    "        # print('Precision:', precision.detach().numpy().min(), precision.detach().numpy().max(), precision.detach().numpy().mean())\n",
    "        # print('Variance:', var.detach().numpy().min(), var.detach().numpy().max(), var.detach().numpy().mean())\n",
    "        # ax[0].hist(precision.sqrt().detach().numpy(), bins=100, color='green', label='precision sqrt')\n",
    "        # ax[1].hist(var.detach().numpy(), bins=100, color='green', label='variance')\n",
    "        # ax[2].hist(r_ui.detach().numpy(), bins=100, color='green', label='relevance')\n",
    "\n",
    "    def guide(self, user, item, Y=None):\n",
    "\n",
    "        # Variational params\n",
    "        ## Obs noise (α in the papers)\n",
    "        # ε = pyro.param('ε', torch.tensor(2), constraint=constraints.positive)\n",
    "        ## Embeddings\n",
    "        µ_u = pyro.param('µ_u', torch.randn(self.n_user, self.embedding_dim) /1000)\n",
    "        µ_i = pyro.param('µ_i', torch.randn(self.n_item, self.embedding_dim) /1000)\n",
    "        ## Embedding vars (regularizer)\n",
    "        α_pu = pyro.param('α_pu', torch.tensor(50000.), constraint=constraints.positive)\n",
    "        β_pu = pyro.param('β_pu', torch.tensor(50000.), constraint=constraints.positive)\n",
    "        α_qi = pyro.param('α_qi', torch.tensor(50000.), constraint=constraints.positive)\n",
    "        β_qi = pyro.param('β_qi', torch.tensor(50000.), constraint=constraints.positive)\n",
    "        ## CPMF gamma hyperparams - Related to precision\n",
    "        α_γu = pyro.param('α_γu', torch.zeros(self.n_user)+5000., constraint=constraints.positive)\n",
    "        β_γu = pyro.param('β_γu', torch.zeros(self.n_user)+1000., constraint=constraints.positive)\n",
    "        α_γi = pyro.param('α_γi', torch.zeros(self.n_item)+5000., constraint=constraints.positive)\n",
    "        β_γi = pyro.param('β_γi', torch.zeros(self.n_item)+1000., constraint=constraints.positive)\n",
    "        \n",
    "        # Hyperpriors (α_u and α_v in the papers)\n",
    "        τ_pu = pyro.sample('τ_pu', dist.Gamma(α_pu, β_pu))\n",
    "        τ_qi = pyro.sample('τ_qi', dist.Gamma(α_qi, β_qi))\n",
    "\n",
    "        # Priors\n",
    "        γ_u = pyro.sample('γ_u', dist.Gamma(α_γu, β_γu).to_event(1))\n",
    "        γ_i = pyro.sample('γ_i', dist.Gamma(α_γi, β_γi).to_event(1))\n",
    "        with pyro.plate('Users'):\n",
    "            p_u = pyro.sample('p_u', dist.Normal(µ_u, 1/τ_pu.sqrt()).to_event(1))\n",
    "        with pyro.plate('Items'):\n",
    "            q_i = pyro.sample('q_i', dist.Normal(µ_i, 1/τ_qi.sqrt()).to_event(1))\n",
    "\n",
    "        with pyro.plate('Relevance', len(user)):\n",
    "            mean = (p_u[user] * q_i[item]).sum(1)\n",
    "            precision = γ_u[user] * γ_i[item]\n",
    "            r_ui = pyro.sample('r_ui', dist.Normal(mean, 1/precision.sqrt()))\n",
    "\n",
    "        # # Debug\n",
    "        # f, ax = plt.subplots(ncols=3)\n",
    "        # var = 1 / (precision)\n",
    "        # print('Precision:', precision.detach().numpy().min(), precision.detach().numpy().max(), precision.detach().numpy().mean())\n",
    "        # print('Variance:', var.detach().numpy().min(), var.detach().numpy().max(), var.detach().numpy().mean())\n",
    "        # ax[0].hist(precision.sqrt().detach().numpy(), bins=100, color='green', label='precision sqrt')\n",
    "        # ax[1].hist(var.detach().numpy(), bins=100, color='green', label='variance')\n",
    "        # ax[2].hist(r_ui.detach().numpy(), bins=100, color='green', label='relevance')\n",
    "\n",
    "    def guide2(self, user, item, Y=None):\n",
    "\n",
    "        # Variational params\n",
    "        ## Gammas\n",
    "        µ_γ_u = pyro.param('µ_γ_u', torch.ones(self.n_user))\n",
    "        µ_γ_i = pyro.param('µ_γ_i', torch.ones(self.n_item))\n",
    "        var_gamma_u = pyro.param('var_γ_u', torch.zeros(self.n_user) +.01, constraint=constraints.positive)\n",
    "        var_gamma_i = pyro.param('var_γ_i', torch.zeros(self.n_item) +.01, constraint=constraints.positive)\n",
    "        ## Embeddings\n",
    "        µ_u = pyro.param('µ_u', torch.zeros(self.n_user, self.embedding_dim) +.1)\n",
    "        µ_i = pyro.param('µ_i', torch.zeros(self.n_item, self.embedding_dim) +.1)\n",
    "        ## Embedding vars (regularizer)\n",
    "        mu_alpha_u = pyro.param('µ_alpha_u', torch.tensor(1.))\n",
    "        mu_alpha_i = pyro.param('µ_alpha_i', torch.tensor(1.))\n",
    "        var_alpha_u = pyro.param('var_alpha_u', torch.tensor(.01), constraint=constraints.positive)\n",
    "        var_alpha_i = pyro.param('var_alpha_i', torch.tensor(.01), constraint=constraints.positive)\n",
    "        \n",
    "        # Hyperpriors\n",
    "        ## Embedding vars (regularizer)\n",
    "        α_pu = pyro.sample('α_pu', dist.LogNormal(mu_alpha_u, var_alpha_u))\n",
    "        α_qi = pyro.sample('α_qi', dist.LogNormal(mu_alpha_i, var_alpha_i))\n",
    "        γ_u = pyro.sample('γ_u', dist.LogNormal(µ_γ_u, var_gamma_u).expand([self.n_user]).to_event(1))\n",
    "        γ_i = pyro.sample('γ_i', dist.LogNormal(µ_γ_i, var_gamma_i).expand([self.n_item]).to_event(1))\n",
    "\n",
    "        # Priors\n",
    "        with pyro.plate('Users'):\n",
    "            p_u = pyro.sample('p_u', dist.Normal(µ_u, α_pu).to_event(1))\n",
    "        with pyro.plate('Items'):\n",
    "            q_i = pyro.sample('q_i', dist.Normal(µ_i, α_qi).to_event(1))\n",
    "\n",
    "        with pyro.plate('Relevance', len(user)):\n",
    "            mean = (p_u[user] * q_i[item]).sum(1)\n",
    "            var = torch.exp(γ_u[user] * γ_i[item])\n",
    "            r_ui = pyro.sample('r_ui', dist.Normal(mean, var))\n",
    "\n",
    "    def get_user_embeddings(self, user_ids):\n",
    "        return self.user_embeddings[user_ids], self.user_var[user_ids]\n",
    "    \n",
    "    def get_item_embeddings(self, item_ids=None):\n",
    "        if item_ids is not None:\n",
    "            return self.item_embeddings[item_ids], self.item_var[item_ids]\n",
    "        else:\n",
    "            return self.item_embeddings, self.item_var\n",
    "    \n",
    "    def interact(self, user_embeddings, item_embeddings):\n",
    "        relevance = self.logit((user_embeddings[0] * item_embeddings[0]).sum(1))\n",
    "        unc = 1 / (user_embeddings[1] * item_embeddings[1])\n",
    "        return relevance, unc\n",
    "                \n",
    "\n",
    "pyro.clear_param_store()\n",
    "model = CPMF_like(data.n_user, data.n_item, embedding_dim=2)\n",
    "\n",
    "display(pyro.render_model(model.model, model_args=(data.train[:, 0], \n",
    "                                                   data.train[:, 1], \n",
    "                                                   np.zeros_like(data.train[:, 0])), \n",
    "                          render_params=True, render_distributions=True))\n",
    "\n",
    "# pyro.infer.autoguide.AutoNormal(model.model)\n",
    "display(pyro.render_model(model.guide, model_args=(data.train[:, 0], \n",
    "                                                   data.train[:, 1], \n",
    "                                                   np.ones_like(data.train[:, 0])), \n",
    "                          render_params=True, render_distributions=True))\n",
    "\n",
    "# trace = poutine.trace(model.guide).get_trace(data.train[:, 0], \n",
    "#                                              data.train[:, 1], \n",
    "#                                              np.ones_like(data.train[:, 0]))\n",
    "print(trace.format_shapes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pyro.clear_param_store()\n",
    "train(model, data, n_steps=500, auto_guide=False)\n",
    "#clear_output(wait=True)\n",
    "plt.plot(model.epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in pyro.get_param_store():\n",
    "    print(i, pyro.param(i))\n",
    "\n",
    "rec = model.recommend(3, n=data.n_item)\n",
    "rec['support'] = data.item['support'].loc[rec.index]\n",
    "rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in pyro.get_param_store():\n",
    "    print(i, pyro.param(i))\n",
    "\n",
    "rec = model.recommend(3, n=data.n_item)\n",
    "rec['support'] = data.item['support'].loc[rec.index]\n",
    "rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = test_chap_five(model, data, max_k=10, name='SVI_MF')\n",
    "clear_output(wait=True)\n",
    "results['MAP'].mean(0)[0], results['MAP'].mean(0)[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully bayesian model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uncertain.core import VanillaRecommender, UncertainRecommender\n",
    "from uncertain.implicit.base import Implicit\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import pyro\n",
    "import pyro.poutine as poutine\n",
    "import pyro.distributions as dist\n",
    "import pyro.distributions.constraints as constraints\n",
    "import pyro.optim as optim\n",
    "\n",
    "from pyro.infer import SVI, Trace_ELBO, MCMC, NUTS\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.distributions import constraints\n",
    "\n",
    "\n",
    "class BPMF(Implicit, VanillaRecommender):\n",
    "\n",
    "    def __init__(self, n_user, n_item, embedding_dim, alpha=2):\n",
    "\n",
    "        super().__init__()\n",
    "        self.n_user = n_user\n",
    "        self.n_item = n_item\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.logit = lambda x: 1 / (1 + torch.exp(-x))\n",
    "\n",
    "    def model2(self, user, item, Y=None):\n",
    "\n",
    "        # Hyperparams\n",
    "        mu_0 = pyro.param('µ_0', torch.zeros(self.embedding_dim))\n",
    "        sigma_0 = pyro.param('sigma_0', torch.eye(self.embedding_dim), \n",
    "                             constraint=constraints.positive_definite)\n",
    "\n",
    "        # Hyper-Priors\n",
    "        mu_u = pyro.sample('µ_u', dist.MultivariateNormal(mu_0, sigma_0))\n",
    "        mu_i = pyro.sample('µ_i', dist.MultivariateNormal(mu_0, sigma_0))\n",
    "\n",
    "        # Priors\n",
    "        with pyro.plate('Users', self.n_user):\n",
    "            p_u = pyro.sample('p_u', dist.MultivariateNormal(mu_u, sigma_0))\n",
    "        with pyro.plate('Items', self.n_item):\n",
    "            q_i = pyro.sample('q_i', dist.MultivariateNormal(mu_i, sigma_0))\n",
    "            \n",
    "        with pyro.plate('Relevance', len(user)):\n",
    "            mean = (p_u[user] * q_i[item]).sum(1)\n",
    "            r_ui = pyro.sample('r_ui', dist.Normal(mean, self.alpha))\n",
    "            Y_ui = pyro.sample('Y_ui', dist.Bernoulli(self.logit(r_ui)), obs=torch.tensor(Y).float())\n",
    "        print(mu_u, p_u, mean, r_ui, self.logit(r_ui))\n",
    "        plt.hist(r_ui.detach().numpy())\n",
    "\n",
    "    def model(self, user, item, Y=None):\n",
    "\n",
    "        # Hyperparams\n",
    "        eta_0 = pyro.param('η_0', torch.tensor(1), constraint=constraints.positive)\n",
    "        mu_0 = pyro.param('µ_0', torch.zeros(self.embedding_dim))\n",
    "\n",
    "        # Hyper-Priors\n",
    "        Lambda_u = pyro.sample('Λ_u', dist.LKJCholesky(self.embedding_dim, eta_0))\n",
    "        Lambda_i = pyro.sample('Λ_i', dist.LKJCholesky(self.embedding_dim, eta_0))\n",
    "        mu_u = pyro.sample('µ_u', dist.MultivariateNormal(mu_0, scale_tril=Lambda_u))\n",
    "        mu_i = pyro.sample('µ_i', dist.MultivariateNormal(mu_0, scale_tril=Lambda_i))\n",
    "\n",
    "        # Priors\n",
    "        with pyro.plate('Users', self.n_user):\n",
    "            p_u = pyro.sample('p_u', dist.MultivariateNormal(mu_u, scale_tril=Lambda_u))\n",
    "        with pyro.plate('Items', self.n_item):\n",
    "            q_i = pyro.sample('q_i', dist.MultivariateNormal(mu_i, scale_tril=Lambda_i))\n",
    "            \n",
    "        with pyro.plate('Relevance', len(user)):\n",
    "            mean = (p_u[user] * q_i[item]).sum(1)\n",
    "            r_ui = pyro.sample('r_ui', dist.Normal(mean, self.alpha))\n",
    "            Y_ui = pyro.sample('Y_ui', dist.Bernoulli(self.logit(r_ui)), obs=torch.tensor(Y).float())\n",
    "        #print(mu_u, p_u, mean, r_ui, self.logit(r_ui))\n",
    "        #plt.hist(r_ui.detach().numpy())\n",
    "    \n",
    "    def guide(self, user, item, Y=None):\n",
    "        \n",
    "        # Variational params\n",
    "        mu_u = pyro.param('µ_u_hat', torch.zeros(self.n_user, self.embedding_dim))\n",
    "        mu_i = pyro.param('µ_i_hat', torch.zeros(self.n_item, self.embedding_dim))\n",
    "        p_u = pyro.sample('p_u', dist.Delta(mu_u))\n",
    "        q_i = pyro.sample('q_i', dist.Delta(mu_i))\n",
    "\n",
    "        # with pyro.plate('Users', self.n_user):\n",
    "           \n",
    "        # with pyro.plate('Items', self.n_item):\n",
    "            \n",
    "        with pyro.plate('Relevance', len(user)):\n",
    "            mean = (p_u[user] * q_i[item]).sum(1)\n",
    "            r_ui = pyro.sample('r_ui', dist.Normal(mean, self.alpha))\n",
    "            \n",
    "    def do_inference(self, data, n_negative=2, n_steps=10, learning_rate=.005, auto_guide=False):\n",
    "\n",
    "        if not hasattr(self, 'losses'):\n",
    "            self.epoch_loss = []\n",
    "\n",
    "        self.optimizer = optim.Adam({'lr': learning_rate})\n",
    "        if not auto_guide:\n",
    "            guide = self.guide\n",
    "        else:\n",
    "            guide = pyro.infer.autoguide.AutoNormal(self.model)\n",
    "            # guide = pyro.infer.autoguide.AutoDelta(self.model)\n",
    "        svi = SVI(self.model, guide, self.optimizer, loss=Trace_ELBO())\n",
    "        for _ in (pbar := tqdm(range(n_steps), desc='Training progress')):\n",
    "            \n",
    "            epoch_loss = 0\n",
    "            n_bat = 0\n",
    "            for databat in (pbar2 := tqdm(data.train_dataloader(), leave=False, desc='Epoch progress')):\n",
    "                n_bat += 1\n",
    "                n = len(databat)\n",
    "                user = databat[:, 0]\n",
    "                item = databat[:, 1]\n",
    "                \n",
    "                # Sample negatives\n",
    "                neg_user = np.random.randint(low=0, high=self.n_user, size=n*n_negative)\n",
    "                neg_item = np.random.randint(low=0, high=self.n_item, size=n*n_negative)\n",
    "                users = np.concatenate((user, neg_user))\n",
    "                items = np.concatenate((item, neg_item))\n",
    "                Y = np.concatenate((np.ones_like(user), np.zeros_like(neg_user)))\n",
    "                \n",
    "                # Update\n",
    "                epoch_loss += svi.step(users, items, Y)\n",
    "\n",
    "            self.epoch_loss.append(epoch_loss / n_bat)\n",
    "            pbar.set_postfix({'ELBO loss': self.epoch_loss[-1]})\n",
    "\n",
    "        if not auto_guide:\n",
    "            self.user_embeddings = pyro.param('µ_u_hat').detach()\n",
    "            self.item_embeddings = pyro.param('µ_i_hat').detach()\n",
    "        else:\n",
    "            self.user_embeddings = pyro.param('AutoNormal.locs.p_u').detach()\n",
    "            self.item_embeddings = pyro.param('AutoNormal.locs.q_i').detach()\n",
    "\n",
    "    def get_user_embeddings(self, user_ids):\n",
    "        return self.user_embeddings[user_ids]\n",
    "    \n",
    "    def get_item_embeddings(self, item_ids=None):\n",
    "        if item_ids is not None:\n",
    "            return self.item_embeddings[item_ids]\n",
    "        else:\n",
    "            return self.item_embeddings\n",
    "    \n",
    "    def interact(self, user_embeddings, item_embeddings):\n",
    "        relevance = self.logit((user_embeddings * item_embeddings).sum(1))\n",
    "        return relevance\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "pyro.clear_param_store()\n",
    "model = BPMF(data.n_user, data.n_item, 2, alpha=2)\n",
    "\n",
    "display(pyro.render_model(model.model, model_args=(data.train[:, 0], \n",
    "                                                   data.train[:, 1], \n",
    "                                                   np.zeros_like(data.train[:, 0])), \n",
    "                          render_params=True, render_distributions=True))\n",
    "\n",
    "# display(pyro.render_model(pyro.infer.autoguide.AutoDelta(model.model), model_args=(data.train[:, 0], \n",
    "#                                                    data.train[:, 1], \n",
    "#                                                    np.ones_like(data.train[:, 0])), \n",
    "#                           render_params=True, render_distributions=True))\n",
    "\n",
    "# trace = poutine.trace(pyro.infer.autoguide.AutoDelta(model.model)).get_trace(data.train[:, 0], \n",
    "#                                              data.train[:, 1], \n",
    "#                                              np.ones_like(data.train[:, 0]))\n",
    "print(trace.format_shapes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.clear_param_store()\n",
    "model.do_inference(data, n_steps=50, learning_rate=0.0001, auto_guide=True)\n",
    "clear_output(wait=True)\n",
    "plt.plot(model.epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in pyro.get_param_store():\n",
    "    print(i)\n",
    "\n",
    "rec = model.recommend(3, n=data.n_item)\n",
    "rec['support'] = data.item['support'].loc[rec.index]\n",
    "rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.item_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = test_vanilla(model, data, max_k=10, name='test_pyro')\n",
    "clear_output(wait=True)\n",
    "results['MAP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uncertain.core import VanillaRecommender, UncertainRecommender\n",
    "from uncertain.implicit.base import Implicit\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import pyro\n",
    "import pyro.poutine as poutine\n",
    "import pyro.distributions as dist\n",
    "import pyro.distributions.constraints as constraints\n",
    "import pyro.optim as optim\n",
    "\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "class CPMF_like(Implicit, UncertainRecommender):\n",
    "\n",
    "    def __init__(self, n_user, n_item, embedding_dim, alpha=0.1, alpha_u=0.1, alpha_i=0.1):\n",
    "\n",
    "        super().__init__()\n",
    "        self.n_user = n_user\n",
    "        self.n_item = n_item\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.alpha_u = alpha_u\n",
    "        self.alpha_i = alpha_i\n",
    "\n",
    "        self.logit = lambda x: 1 / (1 + torch.exp(-x))\n",
    "\n",
    "    def model(self, user, item, Y=None):\n",
    "        # Register params\n",
    "        mean_u = pyro.param('µ_u', torch.zeros(self.n_user, self.embedding_dim))\n",
    "        mean_i = pyro.param('µ_i', torch.zeros(self.n_item, self.embedding_dim))\n",
    "        gamma_u = pyro.param('γ_u', torch.ones(self.n_user))\n",
    "        gamma_i = pyro.param('γ_i', torch.ones(self.n_item))\n",
    "        \n",
    "        # Priors\n",
    "        p_u = pyro.sample('p_u', dist.Normal(mean_u, self.alpha_u).to_event(2))\n",
    "        q_i = pyro.sample('q_i', dist.Normal(mean_i, self.alpha_i).to_event(2))\n",
    "\n",
    "        with pyro.plate('relevance plate'):\n",
    "            mean = (p_u[user] * q_i[item]).sum(1)\n",
    "            var = gamma_u[user] * gamma_i[item] * self.alpha\n",
    "            r_ui = pyro.sample('r_ui', dist.Normal(mean, var))\n",
    "            Y_ui = pyro.sample('Y_ui', dist.Bernoulli(self.logit(r_ui)), obs=torch.tensor(Y).float())\n",
    "        \n",
    "    def guide(self, user, item, Y):\n",
    "        # Register params\n",
    "        mean_u = pyro.param('µ_u', torch.normal(0, 0.1, size=(self.n_user, self.embedding_dim)))\n",
    "        mean_i = pyro.param('µ_i', torch.normal(0, 0.1, size=(self.n_item, self.embedding_dim)))\n",
    "        gamma_u = pyro.param('γ_u', torch.ones(self.n_user))\n",
    "        gamma_i = pyro.param('γ_i', torch.ones(self.n_item))\n",
    "        \n",
    "        # Priors\n",
    "        p_u = pyro.sample('p_u', dist.Normal(mean_u, self.alpha_u).to_event(2))\n",
    "        q_i = pyro.sample('q_i', dist.Normal(mean_i, self.alpha_i).to_event(2))\n",
    "\n",
    "        with pyro.plate('relevance plate'):\n",
    "            mean = (p_u[user] * q_i[item]).sum(1)\n",
    "            var = torch.exp(gamma_u[user] * gamma_i[item]) * self.alpha\n",
    "            r_ui = pyro.sample('r_ui', dist.Normal(mean, var))\n",
    "            # Y_ui = pyro.sample('Y_ui', dist.Bernoulli(self.logit(r_ui)), infer={'is_auxiliary': True})\n",
    "\n",
    "    def do_inference(self, data, n_negative=2, n_steps=10, learning_rate=.005):\n",
    "\n",
    "        if not hasattr(self, 'losses'):\n",
    "            self.loss = []\n",
    "\n",
    "        self.optimizer = optim.Adam({'lr': learning_rate})\n",
    "        svi = SVI(self.model, pyro.infer.autoguide.AutoNormal(), self.optimizer, loss=Trace_ELBO())\n",
    "        for _ in (pbar := tqdm(range(n_steps), desc='Training progress')):\n",
    "            # Print user embeddings for sanity check\n",
    "            try:\n",
    "                print(pyro.param('γ_u'))\n",
    "            except:\n",
    "                pass\n",
    "                \n",
    "            for databat in (pbar2 := tqdm(data.train_dataloader(), leave=False, desc='Epoch progress')):\n",
    "                n = len(databat)\n",
    "                user = databat[:, 0]\n",
    "                item = databat[:, 1]\n",
    "                \n",
    "                # Sample negatives\n",
    "                neg_user = np.random.randint(low=0, high=self.n_user, size=n*n_negative)\n",
    "                neg_item = np.random.randint(low=0, high=self.n_item, size=n*n_negative)\n",
    "                users = np.concatenate((user, neg_user))\n",
    "                items = np.concatenate((item, neg_item))\n",
    "                Y = np.concatenate((np.ones_like(user), np.zeros_like(neg_user)))\n",
    "                \n",
    "                # Update\n",
    "                self.loss.append(svi.step(users, items, Y))\n",
    "                pbar.set_postfix({'ELBO loss': self.loss[-1]})\n",
    "\n",
    "        '''# IF CUSTOM GUIDE\n",
    "        self.user_embeddings = pyro.param('µ_u').detach()\n",
    "        self.item_embeddings = pyro.param('µ_i').detach()\n",
    "        self.user_var = pyro.param('γ_u').detach()\n",
    "        self.item_var = pyro.param('γ_i').detach()\n",
    "        '''\n",
    "\n",
    "        # IF AUTO GUIDE\n",
    "        self.user_embeddings = pyro.param('µ_u').detach()\n",
    "        self.item_embeddings = pyro.param('µ_i').detach()\n",
    "        self.user_var = pyro.param('γ_u').detach()\n",
    "        self.item_var = pyro.param('γ_i').detach()\n",
    "\n",
    "    def get_user_embeddings(self, user_ids):\n",
    "        return self.user_embeddings[user_ids], self.user_var[user_ids]\n",
    "    \n",
    "    def get_item_embeddings(self, item_ids=None):\n",
    "        if item_ids is not None:\n",
    "            return self.item_embeddings[item_ids], self.item_var[item_ids]\n",
    "        else:\n",
    "            return self.item_embeddings, self.item_var\n",
    "    \n",
    "    def interact(self, user_embeddings, item_embeddings):\n",
    "        relevance = self.logit((user_embeddings[0] * item_embeddings[0]).sum(1))\n",
    "        unc = torch.exp(user_embeddings[1] * item_embeddings[1]) * self.alpha\n",
    "        return relevance, unc\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "pyro.clear_param_store()\n",
    "model = CPMF_like(data.n_user, data.n_item, 16, alpha=1, alpha_u=1e-10, alpha_i=1e-10)\n",
    "\n",
    "display(pyro.render_model(model.model, model_args=(data.train[:, 0], \n",
    "                                                   data.train[:, 1], \n",
    "                                                   np.ones_like(data.train[:, 0])), \n",
    "                          render_params=True, render_distributions=True))\n",
    "\n",
    "trace = poutine.trace(model.model).get_trace(data.train[:, 0], \n",
    "                                             data.train[:, 1], \n",
    "                                             np.ones_like(data.train[:, 0]))\n",
    "print(trace.format_shapes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.clear_param_store()\n",
    "model.do_inference(data, n_steps=100, learning_rate=0.001)\n",
    "clear_output(wait=True)\n",
    "plt.plot(model.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec = model.recommend(3, n=data.n_item)\n",
    "rec['support'] = data.item['support'].loc[rec.index]\n",
    "rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = test_uncertain(model, data, max_k=10, name='test_pyro')\n",
    "clear_output(wait=True)\n",
    "results['MAP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPMF(Implicit, VanillaRecommender):\n",
    "\n",
    "    def __init__(self, n_user, n_item, embedding_dim):\n",
    "\n",
    "        # Model dimensions\n",
    "        self.n_user = n_user\n",
    "        self.n_item = n_item\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # observation noise\n",
    "        self.alpha = torch.tensor(2.)\n",
    "\n",
    "        # Init optimizer\n",
    "        self.optimizer = optim.Adam({'lr': .05})\n",
    "\n",
    "    def model(self, user, item, Y):        \n",
    "\n",
    "        # Register parameters\n",
    "        ## user\n",
    "        mu0_u_prior = pyro.param('µ0_u', torch.zeros(0))\n",
    "        v0_u_prior = pyro.param('v0_u', torch.tensor(self.embedding_dim))\n",
    "        W0_u_prior = pyro.param('W0_u', torch.eye(self.embedding_dim))\n",
    "        \n",
    "        ## item\n",
    "        mu0_i = pyro.param('µ0_i', self.mu0_i[item])\n",
    "        v0_i = pyro.param('v0_i', self.v0_i)\n",
    "        W0_i = pyro.param('W0_i', self.W0_i[item])\n",
    "        \n",
    "        # Sample priors\n",
    "        with pyro.plate('users', self.n_user):\n",
    "            Lambda_u = pyro.sample('Λ_u', dist.Wishart(self.embedding_dim, \n",
    "                                                       torch.eye(self.embedding_dim)))\n",
    "            mu_u = pyro.sample('µ_u', dist.MultivariateNormal(0, 1/Lambda_u))\n",
    "        with pyro.plate('items', self.n_item):\n",
    "            Lambda_i = pyro.sample('Λ_i', dist.Wishart(self.embedding_dim, \n",
    "                                                       torch.eye(self.embedding_dim)))\n",
    "            mu_i = pyro.sample('µ_i', dist.MultivariateNormal(0, 1/Lambda_i))\n",
    "\n",
    "        # Sample embeddings\n",
    "        with pyro.plate('user_plate', self.n_user):\n",
    "            p_u = pyro.sample('user_embedding', dist.MultivariateNormal(mu_u, Lambda_u))\n",
    "        with pyro.plate('item_plate', self.n_user):\n",
    "            p_u = pyro.sample('item_embedding', dist.MultivariateNormal(mu_i, Lambda_i))\n",
    "        \n",
    "        # Sample relevance and probability of relevance\n",
    "        with pyro.plate('relevance plate'):         \n",
    "            r_ui = pyro.sample('r_ui', dist.Normal((p_u * q_i).sum(1), self.alpha))\n",
    "            Y_ui = pyro.sample('Y_ui', dist.Bernoulli(1/(1+r_ui.exp())), obs=Y)\n",
    "\n",
    "    def guide(self, user, item, Y):\n",
    "\n",
    "        # Register parameters\n",
    "        ## user\n",
    "        mu0_u = pyro.param('µ0_u', self.mu0_u[user])\n",
    "        v0_u = pyro.param('v0_u', self.v0_u)\n",
    "        W0_u = pyro.param('W0_u', self.W0_u[user])\n",
    "        \n",
    "        ## item\n",
    "        mu0_i = pyro.param('µ0_i', self.mu0_i[item])\n",
    "        v0_i = pyro.param('v0_i', self.v0_i)\n",
    "        W0_i = pyro.param('W0_i', self.W0_i[item])\n",
    "    \n",
    "        # Sample priors\n",
    "        Lambda_u = pyro.sample('Λ_u', dist.Wishart(df=v0_u, covariance_matrix=torch.diag(W0_u)))\n",
    "        Lambda_i = pyro.sample('Λ_i', dist.Wishart(df=v0_i, covariance_matrix=torch.diag(W0_i)))\n",
    "        mu_u = pyro.sample('µ_u', dist.Normal(mu0_u, 1/Lambda_u))\n",
    "        mu_i = pyro.sample('µ_i', dist.Normal(mu0_i, 1/Lambda_i))\n",
    "\n",
    "        # Sample embeddings\n",
    "        with pyro.plate('user_plate', self.n_user):\n",
    "            p_u = pyro.sample('user_embedding', dist.MultivariateNormal(mu_u, Lambda_u))\n",
    "        with pyro.plate('item_plate', self.n_user):\n",
    "            p_u = pyro.sample('item_embedding', dist.MultivariateNormal(mu_i, Lambda_i))\n",
    "        \n",
    "        # Sample relevance and probability of relevance\n",
    "        with pyro.plate('relevance plate'):         \n",
    "            r_ui = pyro.sample('r_ui', dist.Normal((p_u * q_i).sum(1), self.alpha))\n",
    "\n",
    "    def do_inference(self, data, n_steps=10):\n",
    "\n",
    "        svi = SVI(self.model, self.guide, self.optimizer, loss=Trace_ELBO())\n",
    "        for _ in (pbar := tqdm(range(n_steps))):\n",
    "            print(self.user_var)\n",
    "            for databat in (pbar2 := tqdm(data.train_dataloader(), leave=False)):\n",
    "                n = len(databat)\n",
    "                user = databat[:, 0]\n",
    "                item = databat[:, 1]\n",
    "                # Sample negatives\n",
    "                neg_user = np.random.randint(low=0, high=self.n_user, size=n*2)\n",
    "                neg_item = np.random.randint(low=0, high=self.n_item, size=n*2)\n",
    "                users = np.concatenate((user, neg_user))\n",
    "                items = np.concatenate((item, neg_item))\n",
    "                Y = np.concatenate((np.ones_like(user), np.zeros_like(neg_user)))\n",
    "                # Update\n",
    "                loss = svi.step(users, items, Y)\n",
    "                pbar.set_postfix({'loss': loss})\n",
    "\n",
    "\n",
    "pyro.clear_param_store()\n",
    "model = BPMF(data.n_user, data.n_item, 128)\n",
    "\n",
    "guide = \n",
    "\n",
    "display(pyro.render_model(model.model, model_args=(data.train[:, 0], \n",
    "                                                    data.train[:, 1], \n",
    "                                                    np.ones_like(data.train[:, 0])), render_params=True))\n",
    "\n",
    "trace = poutine.trace(model.model).get_trace(data.train[:, 0], data.train[:, 1], np.ones_like(data.train[:, 0]))\n",
    "print(trace.format_shapes())\n",
    "\n",
    "pyro.clear_param_store()\n",
    "model.do_inference(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
